{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install -q torch transformers langchain_chroma bitsandbytes langchain langchain_huggingface langchain-community sentence-transformers  pacmap tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git -C ColBERT/ pull || git clone https://github.com/stanford-futuredata/ColBERT.git\n",
    "import sys; sys.path.insert(0, 'ColBERT/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colbert import Indexer, Searcher\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "\n",
    "def set_csv_field_limit():\n",
    "    maxInt = sys.maxsize\n",
    "    while True:\n",
    "        try:\n",
    "            csv.field_size_limit(maxInt)\n",
    "            break\n",
    "        except OverflowError:\n",
    "            maxInt = int(maxInt/10)\n",
    "    return maxInt\n",
    "\n",
    "def load_documents(doc_file):\n",
    "    \"\"\"\n",
    "    Loads the document contents from the first file.\n",
    "\n",
    "    :param doc_file: Path to the document file (document ID <TAB> document contents).\n",
    "    :return: A dictionary {document_id: document_contents}.\n",
    "    \"\"\"\n",
    "    # Set the field size limit first\n",
    "    set_csv_field_limit()\n",
    "\n",
    "    documents = {}\n",
    "    with open(doc_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            if len(row)==0: continue\n",
    "            doc_id, content = row\n",
    "            documents[doc_id] = content\n",
    "    return documents\n",
    "\n",
    "docs = []\n",
    "doc_file = 'meetings.tsv'\n",
    "documents = load_documents(doc_file)\n",
    "documents['doc_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def load_questions_answers(qa_file):\n",
    "    \"\"\"\n",
    "    Loads the questions and corresponding ground truth document IDs.\n",
    "\n",
    "    :param qa_file: Path to the question-answer file (document ID <TAB> question <TAB> answer).\n",
    "    :return: A list of tuples [(document_id, question, answer)].\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(qa_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            doc_id, question, answer = row\n",
    "            qa_pairs.append((doc_id, question, answer))\n",
    "\n",
    "    # random.shuffle(qa_pairs)\n",
    "\n",
    "    return qa_pairs\n",
    "\n",
    "qa_file = 'questions_answers.tsv'  # document ID <TAB> question <TAB> answer\n",
    "qa_pairs = load_questions_answers(qa_file)\n",
    "qa_pairs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = [documents[k] for k in documents.keys()]\n",
    "queries = [q for _, q, _ in qa_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbits = 2   # encode each dimension with 2 bits\n",
    "doc_maxlen = 300 # truncate passages at 300 tokens\n",
    "max_id = 10000\n",
    "\n",
    "index_name = f'meetings.dev.{nbits}bits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ea3210acad4d4bbf27703f7ef98bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Dec 05, 03:55:21] #> Creating directory c:\\Users\\Admin\\OneDrive\\Documents\\GitHub\\RAG\\experiments\\notebook\\indexes/meetings.dev.2bits \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\colbert\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--colbert-ir--colbertv2.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Starting...\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'colbert-ir/colbertv2.0'\n",
    "\n",
    "with Run().context(RunConfig(nranks=1, experiment='notebook')):  # nranks specifies the number of GPUs to use\n",
    "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits, kmeans_niters=4) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.\n",
    "                                                                                # Consider larger numbers for small datasets.\n",
    "\n",
    "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
    "    indexer.index(name=index_name, collection=collection, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer.get_index() # You can get the absolute path of the index, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the searcher using its relative name (i.e., not a full path), set\n",
    "# experiment=value_used_for_indexing in the RunConfig.\n",
    "with Run().context(RunConfig(experiment='notebook')):\n",
    "    searcher = Searcher(index=index_name, collection=collection)\n",
    "\n",
    "\n",
    "# If you want to customize the search latency--quality tradeoff, you can also supply a\n",
    "# config=ColBERTConfig(ncells=.., centroid_score_threshold=.., ndocs=..) argument.\n",
    "# The default settings with k <= 10 (1, 0.5, 256) gives the fastest search,\n",
    "# but you can gain more extensive search by setting larger values of k or\n",
    "# manually specifying more conservative ColBERTConfig settings (e.g. (4, 0.4, 4096))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how did project manager and user interface introduce the prototype of the remote control ?'\n",
    "print(f\"#> {query}\")\n",
    "\n",
    "# Find the top-3 passages for this query\n",
    "results = searcher.search(query, k=3)\n",
    "\n",
    "# Print out the top-k retrieved passages\n",
    "for passage_id, passage_rank, passage_score in zip(*results):\n",
    "    print(f\"\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
