{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fwmBoWsumvZ"
   },
   "source": [
    "## Retrieval-Augmented Generation (RAG) (40 points)\n",
    "\n",
    "The goal of this assignment is to gain hands-on experience with aspects of **Retrieval-Augmented Generation (RAG)**, with a focus on retrieval. You will use **LangChain**, a framework that simplifies integrating external knowledge into generation tasks by:\n",
    "\n",
    "- Implementing various vector databases for efficient neural retrieval. You will use a vector database for storing our memories.\n",
    "- Allowing seamless integration of pretrained text encoders, which you will access via HuggingFace models. You will use a text encoder to get text embeddings for storing in the vector database.\n",
    "\n",
    "**Data**  \n",
    "You will build a retrieval system using the [QMSum Dataset](https://github.com/Yale-LILY/QMSum), a human-annotated benchmark designed for question answering on long meeting transcripts. The dataset includes over 230 meetings across multiple domains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FP65C9N9umva"
   },
   "source": [
    "# RAG Workflow\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) systems involve several interconnected components. Below is a RAG workflow diagram from Hugging Face. Areas highlighted in blue indicate opportunities for system improvement.\n",
    "\n",
    "In this assignment,  we will focus  on the ***Retriever**  so the PA does not cover any processes starting from \"2. Reader\" and below.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LcI9x5HE-EJ"
   },
   "source": [
    "# First,  install the required model dependancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2846,
     "status": "ok",
     "timestamp": 1731284337515,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "kksG1BIyvhh0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install -q torch transformers langchain_chroma bitsandbytes langchain langchain_huggingface langchain-community sentence-transformers  pacmap tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1731284337516,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "pRYKFwaOumva"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Disable huffingface tokenizers parallelism <- should huggingface\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6gIgmDTumvb"
   },
   "source": [
    "# Load the meetings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1731284337516,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "OL_vP4Goumvb",
    "outputId": "352887ad-ad55-44c5-bc7b-cdef34bcbe28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total meetings (docs): 230\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "def set_csv_field_limit():\n",
    "    maxInt = sys.maxsize\n",
    "    while True:\n",
    "        try:\n",
    "            csv.field_size_limit(maxInt)\n",
    "            break\n",
    "        except OverflowError:\n",
    "            maxInt = int(maxInt/10)\n",
    "    return maxInt\n",
    "\n",
    "def load_documents(doc_file):\n",
    "    \"\"\"\n",
    "    Loads the document contents from the first file.\n",
    "\n",
    "    :param doc_file: Path to the document file (document ID <TAB> document contents).\n",
    "    :return: A dictionary {document_id: document_contents}.\n",
    "    \"\"\"\n",
    "    # Set the field size limit first\n",
    "    set_csv_field_limit()\n",
    "\n",
    "    documents = {}\n",
    "    with open(doc_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            if len(row)==0: continue\n",
    "            doc_id, content = row\n",
    "            documents[doc_id] = content\n",
    "    return documents\n",
    "\n",
    "# Load and process the documents\n",
    "docs = []\n",
    "doc_file = 'meetings.tsv'\n",
    "documents = load_documents(doc_file)\n",
    "\n",
    "for doc_id in documents:\n",
    "    doc = Document(page_content=documents[doc_id])\n",
    "    metadata = {'source': doc_id}\n",
    "    doc.metadata = metadata\n",
    "    docs.append(doc)\n",
    "\n",
    "print(f\"Total meetings (docs): {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"project manager: yep . soon as i get this . okay . this is our last meeting . um i 'll go ahead and go through the minutes from the previous meeting . uh and then we 'll have a , the prototype presentation . um then we will um do an evaluation . uh or we 'll see what , what we need to have under the criteria for the evaluation . then we 'll go through the finance and see if we fall within the budget . um then we 'll do the evaluation , and then we can finish up after that with um any changes that we 'll need to make , or hopefully everything will fall right in line . um let 's see , minutes from the last meeting . um we looked at uh the the trends . we had uh the fashion trends that people want a fancy look-and-feel . it was twice as important as anything else . um they liked fruit and vegetables in the new styles . um and a spongy feel . so we were talking about trying to incorporate those into our prototype . um they wanted limited buttons and simplicity . um then we looked at the uh the method for coming up with our own remote . um looking at other other devices . um the ipod , we really liked the look of that . um we also had uh the kid 's remote for a simple idea . um a two part remote , which was what were were originally looking at . uh and then um there was talk of spee uh speech recognition um becoming more uh predominant and easier to use . but i think we 've still decided not to go with that . then we looked at the components um the materials for the case , the different energy sources , the different types of chips , um and made a decision on what we were going to use to make our remote . um and basically how , what were making for the prototype . so i 'm going to leave it at that and let you guys take over . user interface: the prototype discussion . project manager: the prototype yeah . do you need a this ? user interface: no . project manager: okay . industrial designer: can try to plug that in there user interface: there is our remo the banana . marketing: industrial designer: but user interface: um yeah basically we we st went with the colour yellow . um working on the principle of a fruit which was mentioned , it 's basically designed around a banana . project manager: user interface: um but it would be held in such a fashion , marketing: user interface: where it is , obviously it would n't be that floppy 'cause this would be hard plastic . these would be like the rubber , the rubber grips . so that 's so that would hopefully help with grip , or like the ergonomics of it . um but all the controlling would be done with this scroll wheel . you have to use your imagination a little bit . and this here represents the screen , where you , where you 'd go through . project manager: very nice . user interface: and the the simplest functions would be um almost identical to an ipod , where that one way ch through channels , that way th other way through channels . volume up and down . and then to access the more complicated functions you 'd you sorta go , you press that and go through the menus . it 's that that simple . that just represents the infrared uh beam . that 's a simple on and off switch . um i do n't know , we could use the voice . t that blue bits should be yellow , that that 'd be where the batteries would be i suppose . and um that 's about it . it 's as simple as you , we could make it really . industrial designer: right . user interface: is there anything you want to add ? industrial designer: that 's what we have there . that 's plastic . plastic covered with rubber . we might uh add some more underneath here . maybe give it , give it a form . i mean you 're supposed to hold it like that , but um just if you grab it , take it from somewhere , user interface: yeah . project manager: mm-hmm . industrial designer: so yeah , user interface: does n't make much make much difference . industrial designer: you have some rub yeah . user interface: you could work left-handed or right-handed i suppose . industrial designer: exactly , use both . might as well think about user interface: t the actual thing might be smaller . industrial designer: th think about the button as well . like either put either one one on either side or user interface: yeah . project manager: what but what 's that button ? industrial designer: not do it at all . it 's a quick on-off button . user interface: just the on and off . project manager: uh , 'kay . industrial designer: that 's um marketing: industrial designer: yeah i think it 's pretty important . so you do n't have to fiddle with that . project manager: 'kay . industrial designer: right ? um that 's not um project manager: industrial designer: i 'd say a bit smaller would probably be nice . you wan na play with that over there . user interface: yeah . industrial designer: there you go . user interface: it 's you know it 's flimsy 'cause it 's made out of heavy play-doh , marketing: project manager: would you like to uh industrial designer: right . user interface: but marketing: pretty impressive . project manager: well done . user interface: marketing: kind of a banana . user interface: and whether or not it would fall into the cost everything i suppose . with the scroll and the lcd . project manager: well luckily we are going to find out . or not luckily . um do you have a marketing presentation for us . industrial designer: marketing: i do . okay . you guys are gon na help me do an evaluation of the criteria . um . okay . so first i 'll just discuss some of the criteria that i found . just based on the past trend reports that i was looking at earlier . and then we 'll do a group evaluation of the prototype . and then we will calculate the average score to see how we did . um so the criteria we 're gon na be looking at are the complaints um that we heard from the users who were interviewed earlier . so we 're gon na be doing it based on a seven point scale . and one is going to mean true , that we did actually achieve that . with seven being false , we did not achieve that . . okay . so for the first one , we need to decide , did we solved the problem of the users who complained about an ugly remote ? industrial designer: . user interface: project manager: i think it 's definitely different than anything else out there . user interface: marketing: mm . user interface: yeah . project manager: so if they think that what is out there is ugly , then yes i would say , i would say most definitely . marketing: user interface: i would . project manager: it 's bright . user interface: it 's bright . it 's project manager: it still has your traditional black . user interface: it 's curved . it 's not there 's no sharp industrial designer: user interface: angles to it . project manager: yep , not angular . marketing: mm . industrial designer: i 'd say , when it comes to the ergonomics , the form and stuff , yes that 's definitely more beautiful than your average . marketing: industrial designer: however the colour , we do n't have a say in that . marketing: yeah i think the colours detract a little bit . user interface: some people might say it . yeah . industrial designer: that has been , that has been dictated pretty much by the company . project manager: mm . industrial designer: so uh to answer that honestly i would rather say like uh , we have not solved the problem completely with the ugly remote because the colour is ugly , definitely . project manager: yep . marketing: that 's true . yeah . project manager: user interface: yeah . industrial designer: 's nothing you can say about that . i mean i much prefer something like brushed chrome with that form . user interface: yeah . industrial designer: but project manager: yeah something more modern to go a a modern colour to go with the modern form . industrial designer: right . right . it 's different . you do n't want your uh three feet huge lcd dis display in your living room that 's hanging from the wall to be controlled with something like that . marketing: um okay so , do you think , since we this was a a sign criteria , do you think maybe we should put it somewhere in the middle then ? industrial designer: yeah . marketing: does that sound good ? project manager: yeah . user interface: yeah . industrial designer: marketing: what do you think ? three ? four ? project manager: i would say marketing: five ? project manager: four . industrial designer: yeah . marketing: four is fair . okay . project manager: very non-committal , four . marketing: okay , the second one . did we make it simple for new users ? industrial designer: it 's very intuitive , i think yeah . user interface: yeah . i think that was the main aim , one of the main aims that we had . industrial designer: s give it a one . marketing: one , project manager: yeah . marketing: 'kay . okay . um , do the controls now match the operating behaviour of the users ? user interface: uh yeah . 'cause we 've we 've brought it down to basically four controls most common , which are channel and volume . marketing: i 'd say that project manager: mm-hmm . industrial designer: right . user interface: and then the other ones are just a matter of just going , just scrolling further . project manager: s scrolling through and selecting a few . industrial designer: right . so that 's a one . marketing: so one ? project manager: i think that 's a one . marketing: yeah ? okay . okay um the fourth one . how about the problem of a remote being easily lost ? one of the number one complaints . industrial designer: something that big and that yellow you just do n't lose anymore . project manager: user interface: yeah . marketing: whether you want to or not , you 're not gon na lose it . user interface: it 's bright yellow . industrial designer: user interface: bright yellow 's hard to lose . but um if we were to , if we were , that , the speech recognition . that , we could maybe just use that solely for the the finding thing . that was what we 'd we 'd mentioned . project manager: so if we incorporate speech recognition into it then it could user interface: just just to use , to find it when it was lost . but like i said , like i do n't think you 'd lose something so yellow so easily . industrial designer: oops . hmm . user interface: and it 's not gon na fall , like a rectangle would slip down behind things . that 's gon na be a difficult shape to industrial designer: well what project manager: and it is quite bright and user interface: yeah . marketing: user interface: maybe in the middle again , three or four or something ? project manager: uh industrial designer: s marketing: okay . user interface: i mean you know loo losing things is one of those things that people can lose , i mean a million ways . project manager: yeah . user interface: you can pick it up and walk away with it and then you 've lost it . industrial designer: mm . marketing: that 's true . project manager: but if we do go with the , with the speech recognition , then it , then our scale goes up quite a bit i think . marketing: mm . industrial designer: oh yeah . you probably user interface: yeah . project manager: probably two . you know . if we eliminate the fact that you know it 's impossible to guarantee that it 's not gon na be lost then user interface: yeah . industrial designer: mm . project manager: i 'd say two . industrial designer: marketing: project manager: with the speech recognition , which of course may be changed depending on budget . user interface: yeah . industrial designer: y you could add an extra feature actually . which makes this thing raise hell when you remove it too far from the television . user interface: yeah . industrial designer: we could add that but that 's nothing we have thought of so far . project manager: which , which may be cheaper than speech recognition if it were just a industrial designer: yes . user interface: yeah true . but i mean d just those whistling , clapping key rings you have . they 're cheap . marketing: annoying alarm or something ? project manager: industrial designer: it 's it 's marketing: yeah . user interface: so it ca n't be that industrial designer: um the it 's based on this anti anti-theft technology for suitcases and stuff , user interface: expensive . project manager: some sort of proximity user interface: yeah . industrial designer: where you have one piece that 's attached to your luggage , another piece that starts beeping . that ca n't cost much . user interface: yeah . industrial designer: so that can also easily be integrated because these things are small enough to to hide , so you have one piece , you have to glue somewhere behind your stick it behind your tv and the other user interface: stick it on the tv . project manager: pray that you do n't accidentally lose that piece . industrial designer: right . user interface: marketing: industrial designer: that 'd be tough then . well also your remote would uh alarm you if somebody stole you t your television , yeah . ran off with it without taking the beautiful remote control . project manager: user interface: yeah . marketing: so . are we adding one of these two features ? industrial designer: let 's add one of those features and say yes . marketing: gon na say okay . project manager: okay . marketing: so we 're back to a one ? user interface: two . marketing: or a two ? project manager: two . industrial designer: two . marketing: two , 'kay . okay . are we technologically innovative ? industrial designer: uh user interface: i 'd say so . industrial designer: user interface: uh do n't get many mo remote controls with industrial designer: it 's all just user interface: screens on . industrial designer: it 's all just stolen technology when it comes down to user interface: yeah it 's stolen technology . marketing: from ipod yeah . project manager: it 's user interface: but we have . project manager: but there 's not a lot of yellow , there 's not a lotta yellow . industrial designer: right marketing: but for remotes yeah . project manager: course that was n't really industrial designer: right user interface: fa industrial designer: right right . project manager: we were kinda forced to take that colour . marketing: two ? three ? user interface: 'cause it 's stolen . project manager: i do n't know that we are that innovative , to tell you the truth . user interface: no maybe not . industrial designer: yeah not really . marketing: but how many remotes do you see like this ? user interface: project manager: if we added the screaming factor then we go up . industrial designer: marketing: not so many . industrial designer: user interface: marketing: project manager: um i would say we 're probably at four . industrial designer: right . marketing: really ? okay . that 's gon na hurt us . user interface: marketing: okay . um spongy material ? industrial designer: yeah well you have that , kind of , sort of . project manager: we have some spongy , yeah . user interface: yeah as much as as needed , i think . marketing: 'kay . industrial designer: it 's not a one though . project manager: no . industrial designer: one would be the whole thing project manager: yeah . because it 's only got what , these parts are the grips and perhaps the back side the bottom the underneath on the back . industrial designer: to fold and stuff . yeah . user interface: yeah . industrial designer: so that 's a four at most . project manager: probably a four at most . possibly even a five . marketing: and lastly , did we put the fashion in electronics ? project manager: industrial designer: y yes . user interface: yeah . marketing: i 'd say we did . project manager: if your fashion is b is carmen miranda , you betcha . industrial designer: more user interface: yeah . marketing: industrial designer: well the recent fashion is rather displayed in the in the lcd and the way you operate it than the form and the colour , user interface: on the project manager: it 's true . user interface: yeah . industrial designer: but it definitely is user interface: be what we were told , and they 'd say yeah , definitely . industrial designer: . marketing: 'kay . alright . now we just got ta calculate . six eight twelve sixteen . seventeen divided by s user interface: . project manager: seven is marketing: eight . project manager: two point marketing: project manager: two point four ? user interface: is that some long division ? no . project manager: something . marketing: well i have n't done math in years . industrial designer: marketing: what two user interface: marketing: i dunno . user interface: just , i 'm sure there 's a . marketing: okay we 'll say two point four two . right ? how does that look ? industrial designer: i 'm impressed . i ca n't do that without a calculator . user interface: no i ca n't do long marketing: it 's been a while . user interface: very impressive . project manager: and what what is the acceptable criteria ? is there like a scale that we have to hit ? marketing: oh no . they just told me to industrial designer: marketing: pick my own criteria and have you guys evaluate it basically . project manager: alright then . marketing: so that 's that . project manager: okay . well , let 's see . marketing: project manager: now we get to do the budget numbers . you did n't know that you were gon na have a budget . but we do . okay . user interface: yeah . yeah so . you 'd been going a long time dividing that . it 's two point four two eight five se it just keeps going on . marketing: oh my god . user interface: two point four two basically . marketing: okay . yeah we 'll go with that . project manager: so i have here an industrial designer: fifty percent , you 're kidding . marketing: not too shabby . project manager: yeah . industrial designer: p project manager: we want a fifty percent profit on this . oh you ca n't really see that very well . user interface: charge about three hundred quid for it . project manager: twelve and a half euros is what supposed to cost us . okay , so industrial designer: it 's too much . project manager: well let 's see . industrial designer: um project manager: the f the wonder if i can make this industrial designer: uh project manager: what the oh it wo n't let me do that . okay . alright so at top , i do n't know if you guys can read that or not . i ca n't 'cause i do n't have my glasses on , industrial designer: project manager: but so we 've got the energy source . there 's uh four , five , six categories . industrial designer: battery . project manager: we have energy source , electronics , case . then we have case material supplements , interface type , and then button supplements . okay so uh first of all energy source , we picked battery . um and how many batteries do we think this will probably take ? user interface: project manager: probably some e either two or four . industrial designer: two . project manager: two ? like it . industrial designer: at four it 's gon na be too heavy , so that that 's not our problem . people can change it every month . project manager: excellent . industrial designer: they wo n't know until after they bought it . user interface: marketing: project manager: this is consumerism . industrial designer: project manager: alright so for the electronics our choices are simpl simple chip-on-print , regular chip-on-print , advanced chip-on-print , sample sensor , sample speaker . industrial designer: . user interface: we 're advanced chip are we ? industrial designer: that 's the advanced chip-on-print , yeah . project manager: 'kay , we have one of those . 'kay then the case is a probably it 's double curved . industrial designer: double curved , yes . project manager: case materials are industrial designer: plastic . project manager: plastic . um i guess it 's two , since one for the top , one for the bottom . industrial designer: n no . project manager: is that right or is it just one ? industrial designer: no that 's just one . project manager: maybe it 's one because of the industrial designer: it 's just one mo single mould , we can do that . project manager: 'kay . user interface: yeah yeah . marketing: right . project manager: i guess it does n't matter 'cause the price on that one is zero , which is nice . industrial designer: exactly , right . marketing: oh . project manager: special colour ? industrial designer: that 's not a special colour . it 's a specially ugly colour , but it 's not special . marketing: bright yellow . project manager: interface type . we have pushbutton , scroll-wheel interface , integrated scroll-wheel pushbutton , and an lcd display . user interface: marketing: user interface: s industrial designer: s user interface: that 's yeah . project manager: so we actually have the lcd display marketing: user interface: and then project manager: and then is it the integrated or is it user interface: i 'd say the integrated . project manager: yeah . industrial designer: yes unfortunately . project manager: 'kay . button supplement ? special colour ? user interface: mm . project manager: um special form ? special material . industrial designer: we could of course make the buttons wood . project manager: user interface: marketing: industrial designer: say mahogany or so marketing: it 'd look really lovely . project manager: or titanium . industrial designer: mm-hmm or titanium . project manager: they cost us all the same . marketing: yeah . user interface: remote control . project manager: well we only have one button so really we should n't be charged , industrial designer: uh just marketing: industrial designer: project manager: we should n't be charged anything for the the button supplements . user interface: no that 's getting a bit tiny . project manager: um user interface: yeah . marketing: user interface: i 'd ignore that . marketing: leave it blank . project manager: okay . we 're gon na leave that one blank because we run on a lcd and scroll . so our total is fifteen point five . which i believe is industrial designer: yeah that 's too much . project manager: by three euros over . industrial designer: it 's hard to believe . so we 'll go for the hand dynamo huh ? project manager: user interface: marketing: project manager: so the only thing better than um a banana-shaped remote is one that you shake . user interface: if it w what if we completely took out the the one single button we 've got on . marketing: industrial designer: user interface: and just had a scroll wheel interface . and the lcd display . i suppose the lcd c_ display 's the one that 's pushing it up a bit though . project manager: yeah 'cause the marketing: project manager: well 'cause we have to have both right ? user interface: yeah . industrial designer: i mean let 's let 's face it , it also depends on the software on the on the television . user interface: yeah . industrial designer: you can have the the information that this thing transmits be being displayed on the on the screen . project manager: mm-hmm . industrial designer: so s yeah let 's take away the user interface: yeah you could maybe take out the lcd dis display even , industrial designer: yeah . yeah . user interface: if it if it comes up on the computer itsel on the tv itself . industrial designer: right . project manager: so we may not need the lcd display ? user interface: uh that is possible yeah . industrial designer: right . we may not need it . there you go . project manager: well there we go . industrial designer: perfect . project manager: twelve point five . user interface: there we go . marketing: perfect . project manager: okay . so we just remove our marketing: industrial designer: user interface: screen . marketing: project manager: screen here . user interface: make it a bigger dial . industrial designer: user interface: easier to use . even easier to use then . project manager: industrial designer: okay , the user interface: marketing: project manager: besides look at what the lcd does to our lovely remote . user interface: marketing: project manager: back to the design room boys . industrial designer: so we can just take away a heck of a lot of the marketing: user interface: marketing: . industrial designer: there you go . central ? marketing: what 's the blue part ? user interface: that was just industrial designer: oh that 's just user interface: we ran out of yellow . marketing: oh that 's the batteries . industrial designer: yeah . marketing: okay . industrial designer: there you go user interface: there you go . industrial designer: . oops . user interface: even simpler . marketing: looks more like a banana . user interface: yeah . industrial designer: there you go . user interface: for all those fruit lovers out there . industrial designer: one more criteria . project manager: marketing: project manager: okay so the costs under twelve point five euro . was no . we redesigned it . now it 's yes . user interface: yeah . marketing: project manager: next slide . project evaluation . uh project process , satisfaction with , for example , room for creativity , leadership , teamwork , means , new ideas found . um so i guess that let 's see here . i think that perhaps the project evaluation 's just supposed to be completed by me . but i 'd like to hear your thoughts . marketing: project manager: industrial designer: fair enough . user interface: marketing: trying to fill in some time there . project manager: uh h what did you think of our project process ? industrial designer: great . user interface: i think we did yeah i think we did quite well . um industrial designer: yeah . project manager: good . marketing: good teamwork . industrial designer: just half a day , you have a remote . there you go . user interface: yeah . right from the start of the day . project manager: yeah i think user interface: we sort of knew where we were going straight away i thought . project manager: we st we started off a little little weak . our leadership was quite weak in the beginning . marketing: project manager: um um marketing: project manager: but as the day went along we had more idea of what we were doing . um room for creativity ? there was that . um i think we tried a lotta different things and um i think it was um interesting as you guys brought up more um information and studies that we were right on with a lot of those things . um you guys worked together well as a team . and um the means ? which was the whiteboard and the pens . user interface: yeah . we 've used the whiteboard . industrial designer: super super . project manager: i had some problem with the pen i think , but minus your p marketing: minus your powerpoint fiasco . industrial designer: well that 's not my fault . that 's obviously the people i work for uh that work for me , marketing: no i know . i 'm project manager: well marketing: yeah . incom industrial designer: uh they 've just you know user interface: project manager: have a industrial designer: heads are gon na roll , believe me . project manager: we have a list of employees that you would like fired . user interface: marketing: user interface: industrial designer: yes yes . project manager: okay . n new ideas found ? um industrial designer: marketing: mm . kinda . project manager: yes for the remote . maybe no not f for user interface: technology used . project manager: technology . alright . closing . costs are within the budget . project is evaluated . um complete the final questionnaire and meeting summary . that 's it . user interface: excellent . project manager: and i still have to do my minutes for the last meeting . marketing: project manager: actually . um so there will probably be another questionnaire coming up . and then we 'll have to check with the main boss whether we can , what goes on after that . marketing: we might have a while though . industrial designer: . project manager: but that 's the end of our meeting .\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['doc_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfFRAwGPumvd"
   },
   "source": [
    "# Retriever - Building the retriever 🗂️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RzIq2C3umvd"
   },
   "source": [
    "The **retriever functions like a search engine**: given a user query, it returns relevant documents from the knowledge base.\n",
    "\n",
    "These documents are then used by the Reader model to generate an answer. In this assignment, however, we are only focusing on the retriever, not the Reader model.\n",
    "\n",
    "**Our goal:** Given a user question, find the most relevant documents from the knowledge base.\n",
    "\n",
    "Key parameters:\n",
    "- `top_k`: The number of documents to retrieve. Increasing `top_k` can improve the chances of retrieving relevant content.\n",
    "- `chunk size`: The length of each document. While this can vary, avoid overly long documents, as too many tokens can overwhelm most reader models.\n",
    "\n",
    "\n",
    "Langchain __offers a huge variety of options for vector databases and allows us to keep document metadata throughout the processing__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9m7pdV-Xumvd"
   },
   "source": [
    " ### 1. Specify an Embedding Model and Visualize Document Lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501,
     "referenced_widgets": [
      "01f33dd22cde4935b251fc1ccec28b06",
      "dbaa70ee635e4e8ebdf55c00449ba804",
      "cdbfbc4cf6f74e7e885c147e3aa2f33c",
      "9aa28e63d210437e87bbad2c5faea550",
      "424c827f13d346d7a1aef2e40b3ed31d",
      "aa567554981c449e9a19d2cc80a15a18",
      "5a1b783c75ab49af9e428085777d5c2b",
      "66e6956c50b64d9ba8fb03e8fc38471c",
      "b81068c5e79f4697b1ec9033c2472a2f",
      "885282abe8984dd589b5e8b271c70351",
      "c54bd6023b894bc18350e7f99cb56b3e"
     ]
    },
    "executionInfo": {
     "elapsed": 23646,
     "status": "ok",
     "timestamp": 1731284361159,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "s0fGIq3zumvd",
    "outputId": "8d49ed64-8250-4e2c-e20e-123259911edd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's maximum sequence length: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e12248708a04f05adf2d1688024d06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\n",
    "    f\"Model's maximum sequence length: {SentenceTransformer(EMBEDDING_MODEL_NAME).max_seq_length}\"\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgQzRWqNumvd"
   },
   "source": [
    "### 2. Split the Documents into Chunks\n",
    "\n",
    "The documents (meeting transcripts) are very long—some up to 30,000 tokens! To make retrieval effective, we’ll **split each document into smaller, semantically meaningful chunks**. These chunks will serve as the snippets the retriever compares to the query, returning the `top_k` most relevant ones.\n",
    "\n",
    "**Objective**: Create Semantically Relevant Snippets\n",
    "\n",
    "Chunks should be long enough to capture complete ideas but not so lengthy that they lose focus.\n",
    "\n",
    "We will use Langchain's implementation of recursive chunking with `RecursiveCharacterTextSplitter`.\n",
    "- Parameter `chunk_size` controls the length of individual chunks: this length is counted by default as the number of characters in the chunk.\n",
    "- Parameter `chunk_overlap` lets adjacent chunks get a bit of overlap on each other. This reduces the probability that an idea could be cut in half by the split between two adjacent chunks.\n",
    "\n",
    "From the produced plot below, you can see that now the chunk length distribution looks better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501,
     "referenced_widgets": [
      "75768aac5c9c420e88796c9637557d7a",
      "9708a67aa94a4b0babd63ce89a328d11",
      "0e9edef7b1bc4fddac576d1d6b9080ea",
      "19ba855ec2404856a2cba7ad451b9021",
      "7a33f51157164324a9bf871af54e2140",
      "faf92c79609f41e78b82b2b6f6e9e354",
      "21da30f58191447a8a7a0daa08ace10b",
      "878da6d48d014346bb2460d0276a1647",
      "b01d2f8e35ca4c0c993c70fa59dec499",
      "d4ad90e60abd40a481bf4eed0e4bdf54",
      "01c4b26169344756a5004ae29d7ae15e"
     ]
    },
    "executionInfo": {
     "elapsed": 32792,
     "status": "ok",
     "timestamp": 1731284393949,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "4Ur0Kzz8M5Sj",
    "outputId": "1153b109-efbe-4c82-8950-efbda4b59342"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 18070 snippets to be stored in our vector store.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406f86173e3448e1b8f2d5dd8c217fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18070 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 768,\n",
    "    chunk_overlap = 128,\n",
    ")\n",
    "\n",
    "doc_snippets = text_splitter.split_documents(docs)\n",
    "print(f\"Total {len(doc_snippets)} snippets to be stored in our vector store.\")\n",
    "\n",
    "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(doc_snippets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd8GpdZkumve"
   },
   "source": [
    "### 3. Build the Vector Database\n",
    "\n",
    "To enable retrieval, we need to compute embeddings for all chunks in our knowledge base. These embeddings will then be stored in a vector database.\n",
    "\n",
    "#### How Retrieval Works\n",
    "\n",
    "A query is embedded using an embedding model and a similarity search finds the closest matching chunks in the vector database.\n",
    "\n",
    "The following cell builds the vector database consisting of  all chunks in our knowledge base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93095,
     "status": "ok",
     "timestamp": 1731284487042,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "1GUhbaE-umve",
    "outputId": "568f0c39-f161-42bc-8bd4-4eae3b6c771e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found device: cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "# Automatically set the device to 'cuda' if available, otherwise use 'cpu'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Found device: {device}\")\n",
    "\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "    doc_snippets, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = (end_time - start_time)/60\n",
    "print(f\"Time taken: {elapsed_time} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lXDDWtDumvf"
   },
   "source": [
    "### 4. Querying the Vector Database\n",
    "\n",
    "\n",
    "Using LangChain’s vector database,  the function `vector_database.similarity_search(query)` implements a Bi-Encoder (covered in class), independently encoding the query and each document into a single-vector representation, allowing document embeddings to be precomputed.\n",
    "\n",
    "Let's  define the Bi-Encoder ranking function and then use it on a sample query from the QMSum dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9993,
     "status": "ok",
     "timestamp": 1731284497034,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "LgqBmHxBoZRM",
    "outputId": "e5c04b9a-c162-47c2-9b3b-764caae26312"
   },
   "outputs": [],
   "source": [
    "## The function for ranking documents given a query:\n",
    "def rank_documents_biencoder(user_query, top_k = 5):\n",
    "    \"\"\"\n",
    "    Function for document ranking based on the query.\n",
    "\n",
    "    :param query: The query to retrieve documents for.\n",
    "    :return: A list of document IDs ranked based on the query (mocked).\n",
    "    \"\"\"\n",
    "    retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=top_k)\n",
    "    ranked_list = []\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        ranked_list.append(retrieved_docs[i].metadata['source'])\n",
    "\n",
    "    return ranked_list  # ranked document IDs.\n",
    "\n",
    "\n",
    "user_query = \"what did kirsty williams am say about her plan for quality assurance ?\"\n",
    "retrieved_docs = rank_documents_biencoder(user_query)\n",
    "\n",
    "print(\"\\n==================================Top-5 documents==================================\")\n",
    "print(\"\\n\\nRetrieved documents:\", retrieved_docs)\n",
    "print(\"\\n====================================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lpn_GkMhqyU2"
   },
   "source": [
    "### <font color=\"red\">5. TODO: Implementation of ColBERT as a Reranker for a Bi-Encoder (35 points)</font>\n",
    "\n",
    "The Bi-Encoder’s ranking for the sample query is not optimal: the ground truth document is not ranked at position 1, instead the document ID, **doc_211** is ranked at position 1.  To determine the correct document ID for this query, refer to the `questions_answers.tsv` file.\n",
    "\n",
    "In this task, you will implement the [ColBERT](https://arxiv.org/pdf/2004.12832) approach by Khattab and Zaharia. We’ll use a simplified version of ColBERT, focusing on the following key steps:\n",
    "\n",
    "1. Retrieve the top \\( K = 15 \\) documents for query \\( q \\) using the Bi-Encoder.\n",
    "2. Re-rank these top \\( K = 15 \\) documents using ColBERT's fine-grained interaction scoring. This will involve:\n",
    "   - Using frozen BERT embeddings from a HuggingFace BERT model (no training is required, thus our version is not expected to work as well as full-fledged ColBERT).\n",
    "   - Calculating scores based on fine-grained token-level interactions between the query and each document.\n",
    "3. Implement the method `rank_documents_finegrained_interactions()` to perform this re-ranking.\n",
    "   - Test your method on the same query as in the cell from #4 above.\n",
    "   - Print out the entire re-ranked document list of 5 document IDs, as done in  #4 above (the code below does it for you)\n",
    "4. Ensure that your ColBERT implementation ranks the correct document at position 1 for the sample query.\n",
    "\n",
    "\n",
    "***Note***: Since the same document is divided into multiple chunks that retain the original document ID, you may see the same document ID appear multiple times in your top_k results. However, each instance refers to a different chunk of the document's content.\n",
    "\n",
    "***Note2***:  For this PA we are not focused on query latency, just the late interactions part in the ColBERT approach. Thus, we don't have to pre-compute document matrix representations for ColBERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15001,
     "status": "ok",
     "timestamp": 1731284512033,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "DugEteFxqo8b",
    "outputId": "dc5da8ca-3b28-4760-9d1f-24776c9d3a61"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Load tokenizer and model BERT from HuggingFace\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def rank_documents_finegrained_interactions(user_query, shortlist = 15, top_k=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Rerank the top-K=15 retrieved documents from Bi-encoder using fine-grained token-level interactions\n",
    "    and return the top_k=5 most similar documents.\n",
    "\n",
    "    Args:\n",
    "    - user_query (str): The user query string.\n",
    "    - shortlist (list): Number of documents in the longer short list\n",
    "    - top_k (int): Number of top reranked documents to return.\n",
    "\n",
    "    Returns:\n",
    "    - ranked_list of document IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=shortlist)\n",
    "\n",
    "\n",
    "    # Tokenize the user query\n",
    "    query_inputs = tokenizer(user_query, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "    # Get query token embeddings from BERT\n",
    "    with torch.no_grad():\n",
    "        query_embeddings = model(**query_inputs).last_hidden_state  # Shape: (1, seq_len_query, hidden_dim)\n",
    "\n",
    "    ranked_list = []\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ranked_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        # Tokenize the document content\n",
    "        doc_inputs = tokenizer(doc.page_content, return_tensors='pt', truncation=True, padding=True)\n",
    "        doc_embeddings = model(**doc_inputs).last_hidden_state\n",
    "\n",
    "        similarity_matrix = torch.matmul(query_embeddings, doc_embeddings.transpose(-2, -1))\n",
    "\n",
    "        max_similarities = torch.max(similarity_matrix, dim=-1).values\n",
    "        doc_score = max_similarities.sum().item()\n",
    "        # mean_similarities = torch.mean(similarity_matrix, dim=-1)\n",
    "        # doc_score = mean_similarities.sum().item()\n",
    "\n",
    "        ranked_scores.append([doc_score, doc.metadata['source']])\n",
    "\n",
    "    ranked_scores.sort(reverse=True)\n",
    "\n",
    "    ranked_list = [ranked_scores[i][1] for i in range(top_k)]\n",
    "\n",
    "\n",
    "    return ranked_list  # ranked document IDs\n",
    "\n",
    "\n",
    "user_query = \"how did project manager and user interface introduce the prototype of the remote control ?\"\n",
    "retrieved_docs = rank_documents_finegrained_interactions(user_query)\n",
    "\n",
    "print(\"\\n==================================Top-5 documents==================================\")\n",
    "print(\"\\n\\nRetrieved documents:\", retrieved_docs)\n",
    "print(\"\\n====================================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Full evaluation pipeline for your own exploration. \n",
    "Below is a more complete evaluation setup that works with all the queries in QMSum dataset, and  reports the  `precision@k=5` metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1731284512033,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "f0pjYhz1umvj"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_questions_answers(qa_file):\n",
    "    \"\"\"\n",
    "    Loads the questions and corresponding ground truth document IDs.\n",
    "\n",
    "    :param qa_file: Path to the question-answer file (document ID <TAB> question <TAB> answer).\n",
    "    :return: A list of tuples [(document_id, question, answer)].\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(qa_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            doc_id, question, answer = row\n",
    "            qa_pairs.append((doc_id, question, answer))\n",
    "\n",
    "    random.shuffle(qa_pairs)\n",
    "\n",
    "    return qa_pairs\n",
    "\n",
    "def precision_at_k(ground_truth, retrieved_docs, k):\n",
    "    \"\"\"\n",
    "    Computes Precision at k for a single query.\n",
    "\n",
    "    :param ground_truth: The name of the ground truth document.\n",
    "    :param retrieved_docs: The list of document names returned by the model in ranked order.\n",
    "    :param k: The cutoff for computing Precision.\n",
    "    :return: Precision at k.\n",
    "    \"\"\"\n",
    "    return 1 if ground_truth in retrieved_docs[:k] else 0\n",
    "\n",
    "def evaluate(doc_file, qa_pairs, ranking_fuction = None, k= 5):\n",
    "    \"\"\"\n",
    "    Evaluate the retrieval system based on the documents and question-answer pairs.\n",
    "\n",
    "    :param doc_file: Path to the document file.\n",
    "    :param qa_file: Path to the question-answer file.\n",
    "    :param k: The cutoff for Precision@k.\n",
    "    \"\"\"\n",
    "    # Load the QA pairs\n",
    "\n",
    "\n",
    "    precision_scores = []\n",
    "\n",
    "\n",
    "    for doc_id, question, _ in qa_pairs:\n",
    "\n",
    "        retrieved_docs = ranking_fuction(question)\n",
    "        precision_scores.append(precision_at_k(doc_id, retrieved_docs, k))\n",
    "\n",
    "        avg_precision_at_k = sum(precision_scores) / len(precision_scores)\n",
    "\n",
    "        if len(precision_scores) %10==0:\n",
    "            print(f\"After {len(precision_scores)} queries, Precision@{k}: {avg_precision_at_k}\")\n",
    "\n",
    "    # Compute average Precision@k\n",
    "    avg_precision_at_k = sum(precision_scores) / len(precision_scores)\n",
    "\n",
    "    print(f\"Precision@{k}: {avg_precision_at_k}\")\n",
    "\n",
    "\n",
    "qa_file = 'questions_answers.tsv'  # document ID <TAB> question <TAB> answer\n",
    "qa_pairs = load_questions_answers(qa_file)\n",
    "print(len(qa_pairs))\n",
    "qa_pairs[:3]\n",
    "\n",
    "# start_time = time.time()\n",
    "# evaluate(doc_file, qa_pairs,rank_documents_biencoder)\n",
    "# end_time = time.time()\n",
    "# elapsed_time = (end_time - start_time)/60\n",
    "# print(f\"Time taken: {elapsed_time} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def batch_encode_queries_v2(queries, embedding_model, batch_size=256):\n",
    "    \"\"\"\n",
    "    Optimized batch encoding with larger batches and better GPU utilization\n",
    "    \"\"\"\n",
    "    # Pre-allocate memory for all embeddings\n",
    "    num_queries = len(queries)\n",
    "    embedding_dim = 384  # We know this from the output\n",
    "    all_embeddings = np.zeros((num_queries, embedding_dim), dtype=np.float32)\n",
    "    \n",
    "    # Process in larger batches\n",
    "    for i in tqdm(range(0, num_queries, batch_size), desc=\"Encoding queries\"):\n",
    "        end_idx = min(i + batch_size, num_queries)\n",
    "        batch = queries[i:end_idx]\n",
    "        \n",
    "        # Get embeddings for batch\n",
    "        batch_embeddings = embedding_model.embed_documents(batch)\n",
    "        all_embeddings[i:end_idx] = batch_embeddings\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "def evaluate_gpu_optimized_v2(qa_pairs, ks=[1, 5, 10, 15, 20], batch_size=256, search_batch_size=512):\n",
    "    \"\"\"\n",
    "    Optimized GPU evaluation with precision@k for multiple k values\n",
    "    \"\"\"\n",
    "    questions = [q for _, q, _ in qa_pairs]\n",
    "    ground_truths = [doc_id for doc_id, _, _ in qa_pairs]\n",
    "    max_k = max(ks)  # Use maximum k for retrieval\n",
    "    \n",
    "    print(f\"Starting evaluation with batch_size={batch_size}, search_batch_size={search_batch_size}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Batch encode queries\n",
    "    print(\"Encoding queries...\")\n",
    "    query_embeddings = batch_encode_queries_v2(questions, embedding_model, batch_size)\n",
    "    encoding_time = time.time() - start_time\n",
    "    print(f\"Encoding completed in {encoding_time:.1f} seconds\")\n",
    "    \n",
    "    # 2. Batch similarity search\n",
    "    print(\"Performing batch similarity search...\")\n",
    "    search_start = time.time()\n",
    "    \n",
    "    all_D = []\n",
    "    all_I = []\n",
    "    num_queries = len(questions)\n",
    "    \n",
    "    for i in tqdm(range(0, num_queries, search_batch_size), desc=\"Searching\"):\n",
    "        end_idx = min(i + search_batch_size, num_queries)\n",
    "        batch_embeddings = query_embeddings[i:end_idx]\n",
    "        \n",
    "        # Retrieve max_k documents\n",
    "        D, I = KNOWLEDGE_VECTOR_DATABASE.index.search(batch_embeddings, max_k)\n",
    "        all_D.extend(D)\n",
    "        all_I.extend(I)\n",
    "    \n",
    "    search_time = time.time() - search_start\n",
    "    print(f\"Search completed in {search_time:.1f} seconds\")\n",
    "    \n",
    "    # 3. Process results\n",
    "    doc_dict = {i: doc.metadata['source'] for i, doc in enumerate(doc_snippets)}\n",
    "    retrieved_docs = [[doc_dict[idx] for idx in query_indices] for query_indices in all_I]\n",
    "    \n",
    "    # Calculate precision for each k\n",
    "    precision_scores = {}\n",
    "    for k in ks:\n",
    "        scores = [\n",
    "            1 if gt in retrieved[:k] else 0 \n",
    "            for gt, retrieved in zip(ground_truths, retrieved_docs)\n",
    "        ]\n",
    "        precision_scores[k] = np.mean(scores)\n",
    "    \n",
    "    # Calculate timing metrics\n",
    "    total_time = time.time() - start_time\n",
    "    qps = num_queries / total_time\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nPerformance Breakdown:\")\n",
    "    print(f\"- Encoding time: {encoding_time:.1f}s ({num_queries/encoding_time:.1f} queries/s)\")\n",
    "    print(f\"- Search time: {search_time:.1f}s ({num_queries/search_time:.1f} queries/s)\")\n",
    "    print(f\"\\nPrecision Results:\")\n",
    "    for k in ks:\n",
    "        print(f\"Precision@{k}: {precision_scores[k]:.3f}\")\n",
    "    print(f\"\\nTiming:\")\n",
    "    print(f\"Total time: {total_time:.1f} seconds\")\n",
    "    print(f\"Average speed: {qps:.1f} queries/second\")\n",
    "    \n",
    "    return precision_scores\n",
    "\n",
    "\n",
    "qa_file = 'questions_answers.tsv'\n",
    "qa_pairs = load_questions_answers(qa_file)\n",
    "\n",
    "start_time = time.time()\n",
    "precision_scores = evaluate_gpu_optimized_v2(\n",
    "    qa_pairs, \n",
    "    ks=[1,2,3,4,5],\n",
    "    batch_size=256,\n",
    "    search_batch_size=512\n",
    ")\n",
    "print(f\"Total evaluation time: {(time.time() - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified ColBERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# model = AutoModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialogRPT-updown\")\n",
    "# model = AutoModel.from_pretrained(\"microsoft/DialogRPT-updown\")\n",
    "\n",
    "\n",
    "def batch_initial_retrieval_v2(questions, k=15, batch_size=512):\n",
    "    \"\"\"\n",
    "    More optimized batch retrieval using FAISS GPU\n",
    "    \"\"\"\n",
    "    # Convert FAISS index to GPU if not already\n",
    "    res = faiss.StandardGpuResources()\n",
    "    gpu_index = faiss.index_cpu_to_gpu(res, 0, KNOWLEDGE_VECTOR_DATABASE.index)\n",
    "    \n",
    "    # Pre-allocate embeddings array\n",
    "    num_queries = len(questions)\n",
    "    embedding_dim = KNOWLEDGE_VECTOR_DATABASE.index.d\n",
    "    all_embeddings = np.zeros((num_queries, embedding_dim), dtype=np.float32)\n",
    "    \n",
    "    print(\"Computing embeddings...\")\n",
    "    for i in tqdm(range(0, num_queries, batch_size)):\n",
    "        end_idx = min(i + batch_size, num_queries)\n",
    "        batch = questions[i:end_idx]\n",
    "        \n",
    "        # Get embeddings for batch\n",
    "        embeddings = embedding_model.embed_documents(batch)\n",
    "        all_embeddings[i:end_idx] = embeddings\n",
    "    \n",
    "    print(\"Performing batch search...\")\n",
    "    # Single batch search for all queries\n",
    "    D, I = gpu_index.search(all_embeddings, k)\n",
    "    \n",
    "    # Convert indices to documents (in batches)\n",
    "    retrieved_docs = []\n",
    "    for indices in I:\n",
    "        docs = [doc_snippets[idx] for idx in indices]\n",
    "        retrieved_docs.append(docs)\n",
    "    \n",
    "    return retrieved_docs\n",
    "\n",
    "def optimized_colbert_rerank_v3(questions, docs_list, max_k=20, batch_size=64):\n",
    "    \"\"\"\n",
    "    Optimized ColBERT reranking that maintains high precision\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    all_reranked = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(questions), batch_size), desc=\"Reranking\"):\n",
    "        batch_questions = questions[i:i + batch_size]\n",
    "        batch_docs = docs_list[i:i + batch_size]\n",
    "        \n",
    "        for q_idx, (question, docs) in enumerate(zip(batch_questions, batch_docs)):\n",
    "            # Get query embeddings\n",
    "            q_inputs = tokenizer(\n",
    "                question,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_embeds = model(**q_inputs).last_hidden_state\n",
    "            \n",
    "            doc_scores = []\n",
    "            doc_batch_size = 8\n",
    "            \n",
    "            for j in range(0, len(docs), doc_batch_size):\n",
    "                doc_batch = docs[j:j + doc_batch_size]\n",
    "                doc_texts = [d.page_content for d in doc_batch]\n",
    "                \n",
    "                d_inputs = tokenizer(\n",
    "                    doc_texts,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=512\n",
    "                ).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    d_embeds = model(**d_inputs).last_hidden_state\n",
    "                \n",
    "                for doc_idx in range(len(doc_batch)):\n",
    "                    d_embed = d_embeds[doc_idx:doc_idx+1]\n",
    "                    sim_matrix = torch.matmul(q_embeds, d_embed.transpose(-2, -1))\n",
    "                    max_sim = torch.max(sim_matrix, dim=-1).values\n",
    "                    score = max_sim.sum().item()\n",
    "                    doc_scores.append((score, doc_batch[doc_idx].metadata['source']))\n",
    "            \n",
    "            # Sort and get top max_k docs instead of just top 5\n",
    "            doc_scores.sort(reverse=True)\n",
    "            all_reranked.append([score[1] for score in doc_scores[:max_k]])\n",
    "    \n",
    "    return all_reranked\n",
    "\n",
    "def evaluate_optimized_v3(qa_pairs, initial_k=20, ks=[1, 5, 10, 15, 20], batch_size=512, rerank_batch_size=64):\n",
    "    max_k = max(ks)\n",
    "    initial_k = max(initial_k, max_k)\n",
    "    \n",
    "    questions = [q for _, q, _ in qa_pairs]\n",
    "    ground_truths = [doc_id for doc_id, _, _ in qa_pairs]\n",
    "    \n",
    "    print(\"1. Initial retrieval (batched)...\")\n",
    "    init_start = time.time()\n",
    "    initial_retrieved = batch_initial_retrieval_v2(\n",
    "        questions, \n",
    "        k=initial_k,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    init_time = time.time() - init_start\n",
    "    \n",
    "    print(\"\\n2. ColBERT reranking...\")\n",
    "    rerank_start = time.time()\n",
    "    reranked_docs = optimized_colbert_rerank_v3(\n",
    "        questions, \n",
    "        initial_retrieved,\n",
    "        max_k=max_k,  # Pass max_k to reranking\n",
    "        batch_size=rerank_batch_size\n",
    "    )\n",
    "    rerank_time = time.time() - rerank_start\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision_scores = {}\n",
    "    for k in ks:\n",
    "        scores = [\n",
    "            1 if gt in reranked[:k] else 0 \n",
    "            for gt, reranked in zip(ground_truths, reranked_docs)\n",
    "        ]\n",
    "        precision_scores[k] = np.mean(scores)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nPerformance Breakdown:\")\n",
    "    print(f\"Initial Retrieval: {init_time:.1f}s ({len(questions)/init_time:.1f} q/s)\")\n",
    "    print(f\"Reranking: {rerank_time:.1f}s ({len(questions)/rerank_time:.1f} q/s)\")\n",
    "    \n",
    "    print(\"\\nPrecision Results:\")\n",
    "    for k in sorted(ks):\n",
    "        print(f\"Precision@{k}: {precision_scores[k]:.3f}\")\n",
    "    \n",
    "    print(\"\\nTiming:\")\n",
    "    print(f\"Total time: {time.time() - init_start:.1f}s\")\n",
    "    print(f\"Average speed: {len(questions)/(time.time() - init_start):.1f} queries/second\")\n",
    "    \n",
    "    return precision_scores\n",
    "\n",
    "\n",
    "# Test with multiple k values\n",
    "scores = evaluate_optimized_v3(\n",
    "    qa_pairs,\n",
    "    initial_k=20,            # Initial retrieval pool size\n",
    "    ks=[1,2,3,4,5],  # K values to evaluate\n",
    "    batch_size=512,         # Initial retrieval batch size\n",
    "    rerank_batch_size=64    # Reranking batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Colbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class ColBERT(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         query_maxlen: int = 32,\n",
    "#         doc_maxlen: int = 180,\n",
    "#         dim: int = 128,\n",
    "#         mask_punctuation: bool = True,\n",
    "#         skip_compression: bool = False\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.query_maxlen = query_maxlen\n",
    "#         self.doc_maxlen = doc_maxlen\n",
    "#         self.dim = dim\n",
    "#         self.mask_punctuation = mask_punctuation\n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "#         # Load BERT model and tokenizer\n",
    "#         self.bert = AutoModel.from_pretrained('bert-base-uncased').to(self.device)\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "#         # Linear compression layer\n",
    "#         if not skip_compression:\n",
    "#             self.linear = nn.Linear(self.bert.config.hidden_size, dim, bias=False).to(self.device)\n",
    "#         else:\n",
    "#             self.linear = None\n",
    "            \n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "#         # Pre-compute punctuation IDs\n",
    "#         self.punct_ids = set(self.tokenizer.encode('.!?,', add_special_tokens=False))\n",
    "\n",
    "#     def _mask_punctuation(self, ids: torch.Tensor) -> torch.Tensor:\n",
    "#         \"\"\"Create mask to ignore punctuation tokens\"\"\"\n",
    "#         mask = torch.ones_like(ids, dtype=torch.bool)\n",
    "#         for id in self.punct_ids:\n",
    "#             mask &= (ids != id)\n",
    "#         return mask\n",
    "\n",
    "#     def forward_query(self, query: str) -> torch.Tensor:\n",
    "#         # Tokenize query\n",
    "#         query_tokens = self.tokenizer(\n",
    "#             query,\n",
    "#             max_length=self.query_maxlen,\n",
    "#             padding='max_length',\n",
    "#             truncation=True,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "        \n",
    "#         # Move tensors to correct device\n",
    "#         query_tokens = {k: v.to(self.device) for k, v in query_tokens.items()}\n",
    "        \n",
    "#         # Get BERT embeddings\n",
    "#         with torch.no_grad():\n",
    "#             Q = self.bert(\n",
    "#                 input_ids=query_tokens['input_ids'],\n",
    "#                 attention_mask=query_tokens['attention_mask']\n",
    "#             )[0]\n",
    "        \n",
    "#         Q = self.dropout(Q)\n",
    "#         if self.linear is not None:\n",
    "#             Q = self.linear(Q)\n",
    "#         Q = F.normalize(Q, p=2, dim=2)\n",
    "        \n",
    "#         query_mask = query_tokens['attention_mask'].bool()\n",
    "        \n",
    "#         return Q, query_mask\n",
    "\n",
    "#     def forward_doc(self, doc: str) -> torch.Tensor:\n",
    "#         \"\"\"Encode document tokens\"\"\"\n",
    "#         # Tokenize document with proper padding\n",
    "#         doc_tokens = self.tokenizer(\n",
    "#             doc,\n",
    "#             max_length=self.doc_maxlen,\n",
    "#             padding='max_length',\n",
    "#             truncation=True,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "        \n",
    "#         # Move tensors to correct device\n",
    "#         doc_tokens = {k: v.to(self.device) for k, v in doc_tokens.items()}\n",
    "        \n",
    "#         # Get BERT embeddings\n",
    "#         with torch.no_grad():\n",
    "#             D = self.bert(\n",
    "#                 input_ids=doc_tokens['input_ids'],\n",
    "#                 attention_mask=doc_tokens['attention_mask']\n",
    "#             )[0]  # [batch_size, doc_maxlen, hidden_size]\n",
    "        \n",
    "#         D = self.dropout(D)\n",
    "#         if self.linear is not None:\n",
    "#             D = self.linear(D)\n",
    "#         D = F.normalize(D, p=2, dim=2)\n",
    "        \n",
    "#         # Get document mask\n",
    "#         doc_mask = doc_tokens['attention_mask'].bool()\n",
    "        \n",
    "#         if self.mask_punctuation:\n",
    "#             punct_mask = self._mask_punctuation(doc_tokens['input_ids'])\n",
    "#             doc_mask = doc_mask & punct_mask\n",
    "        \n",
    "#         return D, doc_mask\n",
    "\n",
    "#     def score(self, Q: torch.Tensor, D: torch.Tensor, q_mask: torch.Tensor, d_mask: torch.Tensor) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Calculate MaxSim scores between query and document tokens\n",
    "        \n",
    "#         Args:\n",
    "#             Q: Query embeddings [batch_size, query_len, hidden_dim]\n",
    "#             D: Document embeddings [batch_size, doc_len, hidden_dim]\n",
    "#             q_mask: Query attention mask [batch_size, query_len]\n",
    "#             d_mask: Document attention mask [batch_size, doc_len]\n",
    "#         \"\"\"\n",
    "#         # Ensure all tensors are on the same device\n",
    "#         Q, D = Q.to(self.device), D.to(self.device)\n",
    "#         q_mask, d_mask = q_mask.to(self.device), d_mask.to(self.device)\n",
    "        \n",
    "#         # Compute similarity matrix\n",
    "#         similarity = torch.matmul(Q, D.transpose(-2, -1))  # [batch_size, query_len, doc_len]\n",
    "        \n",
    "#         # Create properly shaped masks\n",
    "#         q_mask = q_mask.unsqueeze(-1)  # [batch_size, query_len, 1]\n",
    "#         d_mask = d_mask.unsqueeze(1)   # [batch_size, 1, doc_len]\n",
    "        \n",
    "#         # Broadcast masks to match similarity matrix shape\n",
    "#         q_mask = q_mask.expand(-1, -1, similarity.size(-1))  # [batch_size, query_len, doc_len]\n",
    "#         d_mask = d_mask.expand(-1, similarity.size(1), -1)   # [batch_size, query_len, doc_len]\n",
    "        \n",
    "#         # Apply masks\n",
    "#         similarity = similarity.masked_fill(~q_mask, -1e9)\n",
    "#         similarity = similarity.masked_fill(~d_mask, -1e9)\n",
    "        \n",
    "#         # MaxSim operation\n",
    "#         scores = similarity.max(dim=-1).values  # [batch_size, query_len]\n",
    "#         scores = scores.sum(dim=-1)  # [batch_size]\n",
    "        \n",
    "#         return scores\n",
    "\n",
    "# class ColBERTRetriever:\n",
    "#     def __init__(self, model_path: str = None):\n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#         self.model = ColBERT().to(self.device)\n",
    "#         if model_path:\n",
    "#             self.model.load_state_dict(torch.load(model_path))\n",
    "#         self.model.eval()\n",
    "\n",
    "#     def retrieve(self, query: str, documents: List[str], top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "#         with torch.no_grad():\n",
    "#             # Encode query\n",
    "#             Q, q_mask = self.model.forward_query(query)\n",
    "            \n",
    "#             scores = []\n",
    "#             # Encode and score each document\n",
    "#             for i, doc in enumerate(documents):\n",
    "#                 D, d_mask = self.model.forward_doc(doc)\n",
    "#                 score = self.model.score(Q, D, q_mask, d_mask).item()\n",
    "#                 scores.append((i, score))\n",
    "            \n",
    "#             scores.sort(key=lambda x: x[1], reverse=True)\n",
    "#             return scores[:top_k]\n",
    "\n",
    "# def rank_documents_colbert(user_query: str, shortlist: int = 15, top_k: int = 5):\n",
    "#     # Get initial candidates from bi-encoder\n",
    "#     retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=shortlist)\n",
    "    \n",
    "#     # Initialize ColBERT retriever\n",
    "#     retriever = ColBERTRetriever()\n",
    "    \n",
    "#     # Get document contents\n",
    "#     documents = [doc.page_content for doc in retrieved_docs]\n",
    "    \n",
    "#     # Rerank using ColBERT\n",
    "#     ranked_indices = retriever.retrieve(user_query, documents, top_k)\n",
    "    \n",
    "#     # Return reranked document IDs\n",
    "#     return [retrieved_docs[idx].metadata['source'] for idx, _ in ranked_indices]\n",
    "\n",
    "# # Test with sample query\n",
    "# user_query = \"what did kirsty williams am say about her plan for quality assurance ?\"\n",
    "# retrieved_docs = rank_documents_colbert(user_query)\n",
    "\n",
    "# print(\"\\n==================================Top-5 documents==================================\")\n",
    "# print(\"\\n\\nRetrieved documents:\", retrieved_docs)\n",
    "# print(\"\\n====================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Callable, Dict\n",
    "\n",
    "\n",
    "# class QAEvaluator:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         batch_size: int = 32,\n",
    "#         device: str = None,\n",
    "#         progress_bar: bool = True\n",
    "#     ):\n",
    "#         self.batch_size = batch_size\n",
    "#         self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#         self.progress_bar = progress_bar\n",
    "        \n",
    "#         # Initialize ColBERT retriever\n",
    "#         self.retriever = ColBERTRetriever()\n",
    "        \n",
    "#         # Enhanced metrics accumulator with additional precision metrics\n",
    "#         self.metrics = {\n",
    "#             'hits@1': [],\n",
    "#             'hits@5': [],\n",
    "#             'mrr': [],\n",
    "#             'recall@5': [],\n",
    "#             'precision@1': [],\n",
    "#             'precision@5': [],\n",
    "#             'precision@10': [],\n",
    "#             'precision@15': [],\n",
    "#             'precision@20': [],\n",
    "#             'latency': []\n",
    "#         }\n",
    "\n",
    "#     def evaluate_batch(\n",
    "#         self,\n",
    "#         queries: List[str],\n",
    "#         doc_ids: List[str],\n",
    "#         shortlist: int = 20,  # Increased to accommodate precision@20\n",
    "#         top_k: int = 20      # Increased to accommodate precision@20\n",
    "#     ) -> Dict[str, List[float]]:\n",
    "#         \"\"\"\n",
    "#         Process a batch of queries and calculate metrics\n",
    "#         \"\"\"\n",
    "#         batch_metrics = {k: [] for k in self.metrics.keys()}\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         # Get initial candidates for all queries in batch\n",
    "#         all_candidates = []\n",
    "#         for query in queries:\n",
    "#             candidates = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=query, k=shortlist)\n",
    "#             all_candidates.append(candidates)\n",
    "\n",
    "#         # Process each query in the batch\n",
    "#         for query, candidates, true_doc_id in zip(queries, all_candidates, doc_ids):\n",
    "#             # Get document contents\n",
    "#             documents = [doc.page_content for doc in candidates]\n",
    "            \n",
    "#             # Get rankings\n",
    "#             ranked_indices = self.retriever.retrieve(query, documents, top_k)\n",
    "#             ranked_docs = [candidates[idx].metadata['source'] for idx, _ in ranked_indices]\n",
    "\n",
    "#             # Calculate existing metrics\n",
    "#             batch_metrics['hits@1'].append(1 if ranked_docs[0] == true_doc_id else 0)\n",
    "#             batch_metrics['hits@5'].append(1 if true_doc_id in ranked_docs[:5] else 0)\n",
    "#             batch_metrics['recall@5'].append(1 if true_doc_id in ranked_docs[:5] else 0)\n",
    "            \n",
    "#             # Calculate MRR\n",
    "#             try:\n",
    "#                 rank = ranked_docs.index(true_doc_id) + 1\n",
    "#                 batch_metrics['mrr'].append(1.0 / rank)\n",
    "#             except ValueError:\n",
    "#                 batch_metrics['mrr'].append(0.0)\n",
    "            \n",
    "#             # Calculate precision@k for different k values\n",
    "#             ks = [1, 5, 10, 15, 20]\n",
    "#             for k in ks:\n",
    "#                 is_relevant = true_doc_id in ranked_docs[:k]\n",
    "#                 batch_metrics[f'precision@{k}'].append(1 if is_relevant else 0)\n",
    "\n",
    "#         # Calculate latency\n",
    "#         batch_time = time.time() - start_time\n",
    "#         batch_metrics['latency'].append(batch_time / len(queries))\n",
    "\n",
    "#         return batch_metrics\n",
    "    \n",
    "#     def evaluate(\n",
    "#         self,\n",
    "#         qa_pairs: List[Tuple[str, str, str]],\n",
    "#         progress_callback: Callable = None\n",
    "#     ) -> Dict[str, float]:\n",
    "#         \"\"\"\n",
    "#         Evaluate the entire dataset\n",
    "#         \"\"\"\n",
    "#         # Split data\n",
    "#         doc_ids = [x[0] for x in qa_pairs]\n",
    "#         queries = [x[1] for x in qa_pairs]\n",
    "        \n",
    "#         # Process in batches\n",
    "#         num_batches = (len(queries) + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "#         if self.progress_bar:\n",
    "#             pbar = tqdm(total=len(queries), desc=\"Evaluating\")\n",
    "        \n",
    "#         for i in range(num_batches):\n",
    "#             start_idx = i * self.batch_size\n",
    "#             end_idx = min((i + 1) * self.batch_size, len(queries))\n",
    "            \n",
    "#             # Get batch\n",
    "#             batch_queries = queries[start_idx:end_idx]\n",
    "#             batch_doc_ids = doc_ids[start_idx:end_idx]\n",
    "            \n",
    "#             # Process batch\n",
    "#             batch_metrics = self.evaluate_batch(\n",
    "#                 queries=batch_queries,\n",
    "#                 doc_ids=batch_doc_ids\n",
    "#             )\n",
    "            \n",
    "#             # Accumulate metrics\n",
    "#             for k, v in batch_metrics.items():\n",
    "#                 self.metrics[k].extend(v)\n",
    "            \n",
    "#             if self.progress_bar:\n",
    "#                 pbar.update(end_idx - start_idx)\n",
    "                \n",
    "#             if progress_callback:\n",
    "#                 progress_callback(i, num_batches, self.get_current_metrics())\n",
    "        \n",
    "#         if self.progress_bar:\n",
    "#             pbar.close()\n",
    "            \n",
    "#         return self.get_current_metrics()\n",
    "\n",
    "#     def get_current_metrics(self) -> Dict[str, float]:\n",
    "#         \"\"\"\n",
    "#         Calculate current metrics\n",
    "#         \"\"\"\n",
    "#         metrics_dict = {\n",
    "#             'hits@1': np.mean(self.metrics['hits@1']),\n",
    "#             'hits@5': np.mean(self.metrics['hits@5']),\n",
    "#             'mrr': np.mean(self.metrics['mrr']),\n",
    "#             'recall@5': np.mean(self.metrics['recall@5']),\n",
    "#             'precision@1': np.mean(self.metrics['precision@1']),\n",
    "#             'precision@5': np.mean(self.metrics['precision@5']),\n",
    "#             'precision@10': np.mean(self.metrics['precision@10']),\n",
    "#             'precision@15': np.mean(self.metrics['precision@15']),\n",
    "#             'precision@20': np.mean(self.metrics['precision@20']),\n",
    "#             'mean_latency': np.mean(self.metrics['latency']),\n",
    "#             'processed_queries': len(self.metrics['mrr'])\n",
    "#         }\n",
    "#         return metrics_dict\n",
    "\n",
    "# def print_progress(batch_num: int, total_batches: int, metrics: Dict[str, float]):\n",
    "#     \"\"\"Optional progress callback with organized output\"\"\"\n",
    "#     print(f\"\\nBatch {batch_num+1}/{total_batches}\")\n",
    "#     print(\"Current metrics:\")\n",
    "    \n",
    "#     # Print original metrics\n",
    "#     print(\"\\nOriginal metrics:\")\n",
    "#     for k in ['hits@1', 'hits@5', 'mrr', 'recall@5']:\n",
    "#         print(f\"{k}: {metrics[k]:.3f}\")\n",
    "    \n",
    "#     # Print precision metrics\n",
    "#     print(\"\\nPrecision metrics:\")\n",
    "#     for k in ['precision@1', 'precision@5', 'precision@10', 'precision@15', 'precision@20']:\n",
    "#         print(f\"{k}: {metrics[k]:.3f}\")\n",
    "    \n",
    "#     print(f\"\\nPerformance:\")\n",
    "#     print(f\"mean_latency: {metrics['mean_latency']:.3f}\")\n",
    "#     print(f\"processed_queries: {metrics['processed_queries']:.0f}\")\n",
    "\n",
    "# # Initialize evaluator\n",
    "# evaluator = QAEvaluator(\n",
    "#     batch_size=32,\n",
    "#     progress_bar=True\n",
    "# )\n",
    "\n",
    "# # Evaluate\n",
    "# results = evaluator.evaluate(\n",
    "#     qa_pairs=qa_pairs,\n",
    "#     progress_callback=print_progress\n",
    "# )\n",
    "\n",
    "# print(\"\\nFinal Results:\")\n",
    "# print(\"==============\")\n",
    "# for metric, value in results.items():\n",
    "#     print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "@dataclass\n",
    "class PromptTemplate:\n",
    "    template: str\n",
    "    input_variables: List[str]\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "class PromptManager:\n",
    "    def __init__(self):\n",
    "        self.templates = {\n",
    "            # Zero-shot prompting\n",
    "            \"basic\": PromptTemplate(\n",
    "                template=\"Answer the question based on the given context.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\",\n",
    "                input_variables=[\"context\", \"question\"]\n",
    "            ),\n",
    "            \n",
    "            # Chain-of-thought prompting\n",
    "            \"cot\": PromptTemplate(\n",
    "                template=\"Let's approach this step-by-step:\\n\\n1) First, understand the question: {question}\\n\\n2) Here's the relevant context: {context}\\n\\n3) Let's analyze the context and break down the key points\\n\\n4) Based on this analysis, provide a detailed answer.\\n\\nReasoning and answer:\",\n",
    "                input_variables=[\"context\", \"question\"]\n",
    "            ),\n",
    "            \n",
    "            # Role-based prompting\n",
    "            \"expert\": PromptTemplate(\n",
    "                template=\"As an expert in meeting analysis, review the following context and answer the question.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nExpert analysis and answer:\",\n",
    "                input_variables=[\"context\", \"question\"]\n",
    "            ),\n",
    "            \n",
    "            # Self-reflection prompting\n",
    "            \"reflective\": PromptTemplate(\n",
    "                template=\"Question: {question}\\n\\nContext: {context}\\n\\nLet me think about this carefully:\\n1. What are the key points in the context?\\n2. How do they relate to the question?\\n3. What might I be missing?\\n\\nConsidering these points, here's my answer:\",\n",
    "                input_variables=[\"context\", \"question\"]\n",
    "            ),\n",
    "            \n",
    "            # Structured output prompting\n",
    "            \"structured\": PromptTemplate(\n",
    "                template=\"Based on the context below, provide a structured answer to the question.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer in the following format:\\n- Main point:\\n- Supporting details:\\n- Additional context:\\n- Confidence level:\\n\",\n",
    "                input_variables=[\"context\", \"question\"]\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def get_prompt(self, style: str, **kwargs) -> str:\n",
    "        if style not in self.templates:\n",
    "            raise ValueError(f\"Unknown prompt style: {style}\")\n",
    "        return self.templates[style].format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  \n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ReaderConfig:\n",
    "    \"\"\"Configuration for the Reader component\"\"\"\n",
    "    model_name: str = \"google/flan-t5-base\"  # Can also use larger variants\n",
    "    max_source_length: int = 1024\n",
    "    max_target_length: int = 256\n",
    "    num_beams: int = 4\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    temperature: float = 0.7\n",
    "    do_sample: bool = True\n",
    "    top_p: float = 0.95\n",
    "    prompt_template: str = \"Answer the question based on the given context.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "\n",
    "class Reader:\n",
    "    def __init__(self, config: ReaderConfig):\n",
    "        \"\"\"Initialize the Reader with a config\"\"\"\n",
    "        self.config = config\n",
    "        self.device = torch.device(config.device)\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        self.model = AutoModelForSeq2SeqGeneration.from_pretrained(config.model_name)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def _prepare_context(self, retrieved_docs: List[str], max_length: Optional[int] = None) -> str:\n",
    "        \"\"\"Prepare context from retrieved documents within length constraints\"\"\"\n",
    "        if not max_length:\n",
    "            max_length = self.config.max_source_length\n",
    "            \n",
    "        # Join documents with separator\n",
    "        context = \" [DOC] \".join([doc.page_content for doc in retrieved_docs])\n",
    "        \n",
    "        # Truncate if needed\n",
    "        tokens = self.tokenizer.encode(context)\n",
    "        if len(tokens) > max_length:\n",
    "            tokens = tokens[:max_length-1] + [tokens[-1]]  # Keep EOS token\n",
    "            context = self.tokenizer.decode(tokens)\n",
    "            \n",
    "        return context\n",
    "    \n",
    "class EnhancedReader(Reader):\n",
    "    def __init__(self, config: ReaderConfig):\n",
    "        super().__init__(config)\n",
    "        self.prompt_manager = PromptManager()\n",
    "        \n",
    "    def generate_answer_with_prompt_style(\n",
    "        self, \n",
    "        question: str, \n",
    "        retrieved_docs: List[str],\n",
    "        prompt_style: str = \"basic\",\n",
    "        return_context: bool = False\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Generate an answer using a specific prompting strategy\n",
    "        \"\"\"\n",
    "        context = self._prepare_context(retrieved_docs, self.config.max_source_length)\n",
    "        \n",
    "        # Get the appropriate prompt\n",
    "        prompt = self.prompt_manager.get_prompt(\n",
    "            style=prompt_style,\n",
    "            context=context,\n",
    "            question=question\n",
    "        )\n",
    "        \n",
    "        # Generate answer with the selected prompt\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=self.config.max_source_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.config.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=self.config.max_target_length,\n",
    "                num_beams=self.config.num_beams,\n",
    "                temperature=self.config.temperature,\n",
    "                do_sample=self.config.do_sample,\n",
    "                top_p=self.config.top_p\n",
    "            )\n",
    "        \n",
    "        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        result = {\n",
    "            \"answer\": answer,\n",
    "            \"prompt_style\": prompt_style\n",
    "        }\n",
    "        \n",
    "        if return_context:\n",
    "            result[\"context\"] = context\n",
    "            result[\"full_prompt\"] = prompt\n",
    "            \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_config = ReaderConfig()\n",
    "reader = EnhancedReader(config=reader_config)\n",
    "\n",
    "rag_output = reader.generate_answer(user_query, retrieved_docs, return_context=True)\n",
    "rag_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "class RAGQAEvaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the RAG QA evaluation system.\"\"\"\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        try:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        except:\n",
    "            print(\"Please install spaCy model: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "    def evaluate_answer(self, \n",
    "                       rag_output: Dict[str, str],\n",
    "                       qa_pairs: List[Tuple[str, str, str]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate RAG output against ground truth QA pairs.\n",
    "        \n",
    "        Args:\n",
    "            rag_output: Dictionary containing 'answer' and 'context' from RAG\n",
    "            qa_pairs: List of tuples (doc_id, question, answer) from ground truth\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Extract mentioned documents from RAG context\n",
    "        mentioned_docs = set(rag_output['context'].split())\n",
    "        \n",
    "        # Find matching QA pairs based on document overlap\n",
    "        matching_pairs = [\n",
    "            (q, a) for doc_id, q, a in qa_pairs \n",
    "            if doc_id in mentioned_docs\n",
    "        ]\n",
    "        \n",
    "        if not matching_pairs:\n",
    "            return {\n",
    "                'error': 'No matching ground truth QA pairs found for the retrieved documents',\n",
    "                'retrieved_docs': list(mentioned_docs)\n",
    "            }\n",
    "        \n",
    "        # Calculate metrics for the generated answer against all matching pairs\n",
    "        answer_metrics = []\n",
    "        for question, answer in matching_pairs:\n",
    "            pair_metrics = self._calculate_metrics(\n",
    "                generated_answer=rag_output['answer'],\n",
    "                reference_question=question,\n",
    "                reference_answer=answer\n",
    "            )\n",
    "            answer_metrics.append(pair_metrics)\n",
    "        \n",
    "        # Take the best scores across all matching pairs\n",
    "        metrics['best_match'] = {\n",
    "            metric: max(m[metric] for m in answer_metrics)\n",
    "            for metric in answer_metrics[0].keys()\n",
    "        }\n",
    "        \n",
    "        # Calculate relevance to retrieved context\n",
    "        if 'context' in rag_output:\n",
    "            context_doc = self.nlp(' '.join(rag_output['context'].split()))\n",
    "            answer_doc = self.nlp(rag_output['answer'])\n",
    "            metrics['context_relevance'] = context_doc.similarity(answer_doc)\n",
    "        \n",
    "        # Add document coverage metrics\n",
    "        metrics['document_coverage'] = {\n",
    "            'num_retrieved': len(mentioned_docs),\n",
    "            'retrieved_docs': list(mentioned_docs)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def _calculate_metrics(self, \n",
    "                         generated_answer: str,\n",
    "                         reference_question: str, \n",
    "                         reference_answer: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate various similarity metrics between generated and reference text.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # ROUGE scores\n",
    "        rouge_scores = self.rouge_scorer.score(reference_answer, generated_answer)\n",
    "        metrics['rouge1_f1'] = rouge_scores['rouge1'].fmeasure\n",
    "        metrics['rouge2_f1'] = rouge_scores['rouge2'].fmeasure\n",
    "        metrics['rougeL_f1'] = rouge_scores['rougeL'].fmeasure\n",
    "        \n",
    "        # BLEU score\n",
    "        metrics['bleu'] = sentence_bleu(\n",
    "            [reference_answer.split()],\n",
    "            generated_answer.split()\n",
    "        )\n",
    "        \n",
    "        # Semantic similarity\n",
    "        ref_answer_doc = self.nlp(reference_answer)\n",
    "        gen_answer_doc = self.nlp(generated_answer)\n",
    "        metrics['semantic_similarity'] = ref_answer_doc.similarity(gen_answer_doc)\n",
    "        \n",
    "        # Question relevance\n",
    "        ref_question_doc = self.nlp(reference_question)\n",
    "        metrics['question_relevance'] = gen_answer_doc.similarity(ref_question_doc)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def get_evaluation_summary(self, metrics: Dict) -> str:\n",
    "        \"\"\"Generate a human-readable summary of the evaluation metrics.\"\"\"\n",
    "        if 'error' in metrics:\n",
    "            return f\"Error: {metrics['error']}\\nRetrieved documents: {', '.join(metrics['retrieved_docs'])}\"\n",
    "        \n",
    "        summary = []\n",
    "        \n",
    "        if 'best_match' in metrics:\n",
    "            summary.append(\"Best Matching Scores:\")\n",
    "            summary.append(f\"Content Overlap:\")\n",
    "            summary.append(f\"- ROUGE-1 F1: {metrics['best_match']['rouge1_f1']:.3f}\")\n",
    "            summary.append(f\"- ROUGE-2 F1: {metrics['best_match']['rouge2_f1']:.3f}\")\n",
    "            summary.append(f\"- ROUGE-L F1: {metrics['best_match']['rougeL_f1']:.3f}\")\n",
    "            summary.append(f\"- BLEU Score: {metrics['best_match']['bleu']:.3f}\")\n",
    "            summary.append(f\"\\nSemantic Evaluation:\")\n",
    "            summary.append(f\"- Semantic Similarity: {metrics['best_match']['semantic_similarity']:.3f}\")\n",
    "            summary.append(f\"- Question Relevance: {metrics['best_match']['question_relevance']:.3f}\")\n",
    "        \n",
    "        if 'context_relevance' in metrics:\n",
    "            summary.append(f\"\\nContext Relevance: {metrics['context_relevance']:.3f}\")\n",
    "        \n",
    "        if 'document_coverage' in metrics:\n",
    "            summary.append(f\"\\nDocument Coverage:\")\n",
    "            summary.append(f\"- Number of Retrieved Documents: {metrics['document_coverage']['num_retrieved']}\")\n",
    "            summary.append(f\"- Retrieved Documents: {', '.join(metrics['document_coverage']['retrieved_docs'])}\")\n",
    "        \n",
    "        return '\\n'.join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluator\n",
    "evaluator = RAGQAEvaluator()\n",
    "\n",
    "# Get evaluation metrics\n",
    "metrics = evaluator.evaluate_answer(rag_output, qa_pairs)\n",
    "\n",
    "# Print human-readable summary\n",
    "print(evaluator.get_evaluation_summary(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cse256pa1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01c4b26169344756a5004ae29d7ae15e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "01f33dd22cde4935b251fc1ccec28b06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dbaa70ee635e4e8ebdf55c00449ba804",
       "IPY_MODEL_cdbfbc4cf6f74e7e885c147e3aa2f33c",
       "IPY_MODEL_9aa28e63d210437e87bbad2c5faea550"
      ],
      "layout": "IPY_MODEL_424c827f13d346d7a1aef2e40b3ed31d"
     }
    },
    "0e9edef7b1bc4fddac576d1d6b9080ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_878da6d48d014346bb2460d0276a1647",
      "max": 18070,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b01d2f8e35ca4c0c993c70fa59dec499",
      "value": 18070
     }
    },
    "19ba855ec2404856a2cba7ad451b9021": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4ad90e60abd40a481bf4eed0e4bdf54",
      "placeholder": "​",
      "style": "IPY_MODEL_01c4b26169344756a5004ae29d7ae15e",
      "value": " 18070/18070 [00:24&lt;00:00, 1796.24it/s]"
     }
    },
    "21da30f58191447a8a7a0daa08ace10b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "424c827f13d346d7a1aef2e40b3ed31d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a1b783c75ab49af9e428085777d5c2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66e6956c50b64d9ba8fb03e8fc38471c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75768aac5c9c420e88796c9637557d7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9708a67aa94a4b0babd63ce89a328d11",
       "IPY_MODEL_0e9edef7b1bc4fddac576d1d6b9080ea",
       "IPY_MODEL_19ba855ec2404856a2cba7ad451b9021"
      ],
      "layout": "IPY_MODEL_7a33f51157164324a9bf871af54e2140"
     }
    },
    "7a33f51157164324a9bf871af54e2140": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "878da6d48d014346bb2460d0276a1647": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "885282abe8984dd589b5e8b271c70351": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9708a67aa94a4b0babd63ce89a328d11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_faf92c79609f41e78b82b2b6f6e9e354",
      "placeholder": "​",
      "style": "IPY_MODEL_21da30f58191447a8a7a0daa08ace10b",
      "value": "100%"
     }
    },
    "9aa28e63d210437e87bbad2c5faea550": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_885282abe8984dd589b5e8b271c70351",
      "placeholder": "​",
      "style": "IPY_MODEL_c54bd6023b894bc18350e7f99cb56b3e",
      "value": " 230/230 [00:19&lt;00:00, 16.52it/s]"
     }
    },
    "aa567554981c449e9a19d2cc80a15a18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b01d2f8e35ca4c0c993c70fa59dec499": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b81068c5e79f4697b1ec9033c2472a2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c54bd6023b894bc18350e7f99cb56b3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cdbfbc4cf6f74e7e885c147e3aa2f33c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66e6956c50b64d9ba8fb03e8fc38471c",
      "max": 230,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b81068c5e79f4697b1ec9033c2472a2f",
      "value": 230
     }
    },
    "d4ad90e60abd40a481bf4eed0e4bdf54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbaa70ee635e4e8ebdf55c00449ba804": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa567554981c449e9a19d2cc80a15a18",
      "placeholder": "​",
      "style": "IPY_MODEL_5a1b783c75ab49af9e428085777d5c2b",
      "value": "100%"
     }
    },
    "faf92c79609f41e78b82b2b6f6e9e354": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
