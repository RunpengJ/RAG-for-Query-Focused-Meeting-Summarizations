{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fwmBoWsumvZ"
   },
   "source": [
    "## Retrieval-Augmented Generation (RAG) (40 points)\n",
    "\n",
    "The goal of this assignment is to gain hands-on experience with aspects of **Retrieval-Augmented Generation (RAG)**, with a focus on retrieval. You will use **LangChain**, a framework that simplifies integrating external knowledge into generation tasks by:\n",
    "\n",
    "- Implementing various vector databases for efficient neural retrieval. You will use a vector database for storing our memories.\n",
    "- Allowing seamless integration of pretrained text encoders, which you will access via HuggingFace models. You will use a text encoder to get text embeddings for storing in the vector database.\n",
    "\n",
    "**Data**  \n",
    "You will build a retrieval system using the [QMSum Dataset](https://github.com/Yale-LILY/QMSum), a human-annotated benchmark designed for question answering on long meeting transcripts. The dataset includes over 230 meetings across multiple domains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FP65C9N9umva"
   },
   "source": [
    "# RAG Workflow\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) systems involve several interconnected components. Below is a RAG workflow diagram from Hugging Face. Areas highlighted in blue indicate opportunities for system improvement.\n",
    "\n",
    "In this assignment,  we will focus  on the ***Retriever**  so the PA does not cover any processes starting from \"2. Reader\" and below.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LcI9x5HE-EJ"
   },
   "source": [
    "# First,  install the required model dependancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2846,
     "status": "ok",
     "timestamp": 1731284337515,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "kksG1BIyvhh0"
   },
   "outputs": [],
   "source": [
    "# pip install -q torch transformers langchain_chroma bitsandbytes langchain langchain_huggingface langchain-community sentence-transformers  pacmap tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1731284337516,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "pRYKFwaOumva"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Disable huffingface tokenizers parallelism <- should huggingface\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6gIgmDTumvb"
   },
   "source": [
    "# Load the meetings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1731284337516,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "OL_vP4Goumvb",
    "outputId": "352887ad-ad55-44c5-bc7b-cdef34bcbe28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total meetings (docs): 230\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "def set_csv_field_limit():\n",
    "    maxInt = sys.maxsize\n",
    "    while True:\n",
    "        try:\n",
    "            csv.field_size_limit(maxInt)\n",
    "            break\n",
    "        except OverflowError:\n",
    "            maxInt = int(maxInt/10)\n",
    "    return maxInt\n",
    "\n",
    "def load_documents(doc_file):\n",
    "    \"\"\"\n",
    "    Loads the document contents from the first file.\n",
    "\n",
    "    :param doc_file: Path to the document file (document ID <TAB> document contents).\n",
    "    :return: A dictionary {document_id: document_contents}.\n",
    "    \"\"\"\n",
    "    # Set the field size limit first\n",
    "    set_csv_field_limit()\n",
    "\n",
    "    documents = {}\n",
    "    with open(doc_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            if len(row)==0: continue\n",
    "            doc_id, content = row\n",
    "            documents[doc_id] = content\n",
    "    return documents\n",
    "\n",
    "# Load and process the documents\n",
    "docs = []\n",
    "doc_file = 'meetings.tsv'\n",
    "documents = load_documents(doc_file)\n",
    "\n",
    "for doc_id in documents:\n",
    "    doc = Document(page_content=documents[doc_id])\n",
    "    metadata = {'source': doc_id}\n",
    "    doc.metadata = metadata\n",
    "    docs.append(doc)\n",
    "\n",
    "print(f\"Total meetings (docs): {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfFRAwGPumvd"
   },
   "source": [
    "# Retriever - Building the retriever üóÇÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RzIq2C3umvd"
   },
   "source": [
    "The **retriever functions like a search engine**: given a user query, it returns relevant documents from the knowledge base.\n",
    "\n",
    "These documents are then used by the Reader model to generate an answer. In this assignment, however, we are only focusing on the retriever, not the Reader model.\n",
    "\n",
    "**Our goal:** Given a user question, find the most relevant documents from the knowledge base.\n",
    "\n",
    "Key parameters:\n",
    "- `top_k`: The number of documents to retrieve. Increasing `top_k` can improve the chances of retrieving relevant content.\n",
    "- `chunk size`: The length of each document. While this can vary, avoid overly long documents, as too many tokens can overwhelm most reader models.\n",
    "\n",
    "\n",
    "Langchain __offers a huge variety of options for vector databases and allows us to keep document metadata throughout the processing__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9m7pdV-Xumvd"
   },
   "source": [
    " ### 1. Specify an Embedding Model and Visualize Document Lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501,
     "referenced_widgets": [
      "01f33dd22cde4935b251fc1ccec28b06",
      "dbaa70ee635e4e8ebdf55c00449ba804",
      "cdbfbc4cf6f74e7e885c147e3aa2f33c",
      "9aa28e63d210437e87bbad2c5faea550",
      "424c827f13d346d7a1aef2e40b3ed31d",
      "aa567554981c449e9a19d2cc80a15a18",
      "5a1b783c75ab49af9e428085777d5c2b",
      "66e6956c50b64d9ba8fb03e8fc38471c",
      "b81068c5e79f4697b1ec9033c2472a2f",
      "885282abe8984dd589b5e8b271c70351",
      "c54bd6023b894bc18350e7f99cb56b3e"
     ]
    },
    "executionInfo": {
     "elapsed": 23646,
     "status": "ok",
     "timestamp": 1731284361159,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "s0fGIq3zumvd",
    "outputId": "8d49ed64-8250-4e2c-e20e-123259911edd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's maximum sequence length: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5016135a365d40e0aacc99b2589c30cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\n",
    "    f\"Model's maximum sequence length: {SentenceTransformer(EMBEDDING_MODEL_NAME).max_seq_length}\"\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs)]\n",
    "\n",
    "# # Plot the distribution of document lengths, counted as the number of tokens\n",
    "# fig = pd.Series(lengths).hist()\n",
    "# plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgQzRWqNumvd"
   },
   "source": [
    "### 2. Split the Documents into Chunks\n",
    "\n",
    "The documents (meeting transcripts) are very long‚Äîsome up to 30,000 tokens! To make retrieval effective, we‚Äôll **split each document into smaller, semantically meaningful chunks**. These chunks will serve as the snippets the retriever compares to the query, returning the `top_k` most relevant ones.\n",
    "\n",
    "**Objective**: Create Semantically Relevant Snippets\n",
    "\n",
    "Chunks should be long enough to capture complete ideas but not so lengthy that they lose focus.\n",
    "\n",
    "We will use Langchain's implementation of recursive chunking with `RecursiveCharacterTextSplitter`.\n",
    "- Parameter `chunk_size` controls the length of individual chunks: this length is counted by default as the number of characters in the chunk.\n",
    "- Parameter `chunk_overlap` lets adjacent chunks get a bit of overlap on each other. This reduces the probability that an idea could be cut in half by the split between two adjacent chunks.\n",
    "\n",
    "From the produced plot below, you can see that now the chunk length distribution looks better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501,
     "referenced_widgets": [
      "75768aac5c9c420e88796c9637557d7a",
      "9708a67aa94a4b0babd63ce89a328d11",
      "0e9edef7b1bc4fddac576d1d6b9080ea",
      "19ba855ec2404856a2cba7ad451b9021",
      "7a33f51157164324a9bf871af54e2140",
      "faf92c79609f41e78b82b2b6f6e9e354",
      "21da30f58191447a8a7a0daa08ace10b",
      "878da6d48d014346bb2460d0276a1647",
      "b01d2f8e35ca4c0c993c70fa59dec499",
      "d4ad90e60abd40a481bf4eed0e4bdf54",
      "01c4b26169344756a5004ae29d7ae15e"
     ]
    },
    "executionInfo": {
     "elapsed": 32792,
     "status": "ok",
     "timestamp": 1731284393949,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "4Ur0Kzz8M5Sj",
    "outputId": "1153b109-efbe-4c82-8950-efbda4b59342"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 18070 snippets to be stored in our vector store.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38da01ac3a5497b90fc18acd5030a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18070 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 768,\n",
    "    chunk_overlap = 128,\n",
    ")\n",
    "\n",
    "doc_snippets = text_splitter.split_documents(docs)\n",
    "print(f\"Total {len(doc_snippets)} snippets to be stored in our vector store.\")\n",
    "\n",
    "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(doc_snippets)]\n",
    "\n",
    "# Plot the distribution of document snippet lengths, counted as the number of tokens\n",
    "# fig = pd.Series(lengths).hist()\n",
    "# plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd8GpdZkumve"
   },
   "source": [
    "### 3. Build the Vector Database\n",
    "\n",
    "To enable retrieval, we need to compute embeddings for all chunks in our knowledge base. These embeddings will then be stored in a vector database.\n",
    "\n",
    "#### How Retrieval Works\n",
    "\n",
    "A query is embedded using an embedding model and a similarity search finds the closest matching chunks in the vector database.\n",
    "\n",
    "The following cell builds the vector database consisting of  all chunks in our knowledge base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93095,
     "status": "ok",
     "timestamp": 1731284487042,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "1GUhbaE-umve",
    "outputId": "568f0c39-f161-42bc-8bd4-4eae3b6c771e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found device: cuda\n",
      "Time taken: 0.3240078091621399 minutes\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "# Automatically set the device to 'cuda' if available, otherwise use 'cpu'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Found device: {device}\")\n",
    "\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "    doc_snippets, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = (end_time - start_time)/60\n",
    "print(f\"Time taken: {elapsed_time} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lXDDWtDumvf"
   },
   "source": [
    "### 4. Querying the Vector Database\n",
    "\n",
    "\n",
    "Using LangChain‚Äôs vector database,  the function `vector_database.similarity_search(query)` implements a Bi-Encoder (covered in class), independently encoding the query and each document into a single-vector representation, allowing document embeddings to be precomputed.\n",
    "\n",
    "Let's  define the Bi-Encoder ranking function and then use it on a sample query from the QMSum dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9993,
     "status": "ok",
     "timestamp": 1731284497034,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "LgqBmHxBoZRM",
    "outputId": "e5c04b9a-c162-47c2-9b3b-764caae26312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================Top-5 documents==================================\n",
      "\n",
      "\n",
      "Retrieved documents: ['doc_211', 'doc_2', 'doc_43', 'doc_160', 'doc_43']\n",
      "\n",
      "====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## The function for ranking documents given a query:\n",
    "def rank_documents_biencoder(user_query, top_k = 5):\n",
    "    \"\"\"\n",
    "    Function for document ranking based on the query.\n",
    "\n",
    "    :param query: The query to retrieve documents for.\n",
    "    :return: A list of document IDs ranked based on the query (mocked).\n",
    "    \"\"\"\n",
    "    retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=top_k)\n",
    "    ranked_list = []\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        ranked_list.append(retrieved_docs[i].metadata['source'])\n",
    "\n",
    "    return ranked_list  # ranked document IDs.\n",
    "\n",
    "\n",
    "user_query = \"what did kirsty williams am say about her plan for quality assurance ?\"\n",
    "retrieved_docs = rank_documents_biencoder(user_query)\n",
    "\n",
    "print(\"\\n==================================Top-5 documents==================================\")\n",
    "print(\"\\n\\nRetrieved documents:\", retrieved_docs)\n",
    "print(\"\\n====================================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lpn_GkMhqyU2"
   },
   "source": [
    "### <font color=\"red\">5. TODO: Implementation of ColBERT as a Reranker for a Bi-Encoder (35 points)</font>\n",
    "\n",
    "The Bi-Encoder‚Äôs ranking for the sample query is not optimal: the ground truth document is not ranked at position 1, instead the document ID, **doc_211** is ranked at position 1.  To determine the correct document ID for this query, refer to the `questions_answers.tsv` file.\n",
    "\n",
    "In this task, you will implement the [ColBERT](https://arxiv.org/pdf/2004.12832) approach by Khattab and Zaharia. We‚Äôll use a simplified version of ColBERT, focusing on the following key steps:\n",
    "\n",
    "1. Retrieve the top \\( K = 15 \\) documents for query \\( q \\) using the Bi-Encoder.\n",
    "2. Re-rank these top \\( K = 15 \\) documents using ColBERT's fine-grained interaction scoring. This will involve:\n",
    "   - Using frozen BERT embeddings from a HuggingFace BERT model (no training is required, thus our version is not expected to work as well as full-fledged ColBERT).\n",
    "   - Calculating scores based on fine-grained token-level interactions between the query and each document.\n",
    "3. Implement the method `rank_documents_finegrained_interactions()` to perform this re-ranking.\n",
    "   - Test your method on the same query as in the cell from #4 above.\n",
    "   - Print out the entire re-ranked document list of 5 document IDs, as done in  #4 above (the code below does it for you)\n",
    "4. Ensure that your ColBERT implementation ranks the correct document at position 1 for the sample query.\n",
    "\n",
    "\n",
    "***Note***: Since the same document is divided into multiple chunks that retain the original document ID, you may see the same document ID appear multiple times in your top_k results. However, each instance refers to a different chunk of the document's content.\n",
    "\n",
    "***Note2***:  For this PA we are not focused on query latency, just the late interactions part in the ColBERT approach. Thus, we don't have to pre-compute document matrix representations for ColBERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15001,
     "status": "ok",
     "timestamp": 1731284512033,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "DugEteFxqo8b",
    "outputId": "dc5da8ca-3b28-4760-9d1f-24776c9d3a61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================Top-5 documents==================================\n",
      "\n",
      "\n",
      "Retrieved documents: ['doc_166', 'doc_9', 'doc_218', 'doc_56', 'doc_15']\n",
      "\n",
      "====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Load tokenizer and model BERT from HuggingFace\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def rank_documents_finegrained_interactions(user_query, shortlist = 15, top_k=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Rerank the top-K=15 retrieved documents from Bi-encoder using fine-grained token-level interactions\n",
    "    and return the top_k=5 most similar documents.\n",
    "\n",
    "    Args:\n",
    "    - user_query (str): The user query string.\n",
    "    - shortlist (list): Number of documents in the longer short list\n",
    "    - top_k (int): Number of top reranked documents to return.\n",
    "\n",
    "    Returns:\n",
    "    - ranked_list of document IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=shortlist)\n",
    "\n",
    "\n",
    "    # Tokenize the user query\n",
    "    query_inputs = tokenizer(user_query, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "    # Get query token embeddings from BERT\n",
    "    with torch.no_grad():\n",
    "        query_embeddings = model(**query_inputs).last_hidden_state  # Shape: (1, seq_len_query, hidden_dim)\n",
    "\n",
    "    ranked_list = []\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ranked_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        # Tokenize the document content\n",
    "        doc_inputs = tokenizer(doc.page_content, return_tensors='pt', truncation=True, padding=True)\n",
    "        doc_embeddings = model(**doc_inputs).last_hidden_state\n",
    "\n",
    "        similarity_matrix = torch.matmul(query_embeddings, doc_embeddings.transpose(-2, -1))\n",
    "\n",
    "        max_similarities = torch.max(similarity_matrix, dim=-1).values\n",
    "        doc_score = max_similarities.sum().item()\n",
    "        # mean_similarities = torch.mean(similarity_matrix, dim=-1)\n",
    "        # doc_score = mean_similarities.sum().item()\n",
    "\n",
    "        ranked_scores.append([doc_score, doc.metadata['source']])\n",
    "\n",
    "    ranked_scores.sort(reverse=True)\n",
    "\n",
    "    ranked_list = [ranked_scores[i][1] for i in range(top_k)]\n",
    "\n",
    "\n",
    "    return ranked_list  # ranked document IDs\n",
    "\n",
    "\n",
    "user_query = \"how did project manager and user interface introduce the prototype of the remote control ?\"\n",
    "retrieved_docs = rank_documents_finegrained_interactions(user_query)\n",
    "\n",
    "print(\"\\n==================================Top-5 documents==================================\")\n",
    "print(\"\\n\\nRetrieved documents:\", retrieved_docs)\n",
    "print(\"\\n====================================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">  7.  (Optional) Full evaluation pipeline for your own exploration. </font>\n",
    "\n",
    "\n",
    "For this assignment, we only ask you to explore  one  sample query.\n",
    "Running on many queries is super slow without the right compute.\n",
    "If you have compute/and/or time to wait, below is a more complete evaluation setup that works with all the queries in QMSum dataset, and  reports the  `precision@k=5` metric.\n",
    "\n",
    "\n",
    "**Note**: you need to remove the comment markers from the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1731284512033,
     "user": {
      "displayName": "Runpeng Jian",
      "userId": "02923340238727093107"
     },
     "user_tz": 480
    },
    "id": "f0pjYhz1umvj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('doc_32',\n",
       "  \"why did n't the team believe that the remote control could fully depend on speech recognition and have no buttons ?\",\n",
       "  'age group data for remote control use was not available ; many people may not want to learn to use the new remote control ; some buttons are still needed , such as channel control , volume settings and on/off .'),\n",
       " ('doc_162',\n",
       "  'what was agreed upon on sample transcripts ?',\n",
       "  'to save time , speaker mn005 will only mark the sample of transcribed data for regions of overlapping speech , as opposed to marking all acoustic events . the digits extraction task will be delegated to whomever is working on acoustics for the meeting recorder project .'),\n",
       " ('doc_116',\n",
       "  'what did user interface propose in the discussion about buttons when discussing the functions ?',\n",
       "  'user interface proposed that there should be six or seven buttons for the same number of categories . users could use these buttons to choose hundreds of channels . these buttons could be navigation buttons .')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def load_questions_answers(qa_file):\n",
    "    \"\"\"\n",
    "    Loads the questions and corresponding ground truth document IDs.\n",
    "\n",
    "    :param qa_file: Path to the question-answer file (document ID <TAB> question <TAB> answer).\n",
    "    :return: A list of tuples [(document_id, question, answer)].\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(qa_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            doc_id, question, answer = row\n",
    "            qa_pairs.append((doc_id, question, answer))\n",
    "\n",
    "    random.shuffle(qa_pairs)\n",
    "\n",
    "    return qa_pairs\n",
    "\n",
    "def precision_at_k(ground_truth, retrieved_docs, k):\n",
    "    \"\"\"\n",
    "    Computes Precision at k for a single query.\n",
    "\n",
    "    :param ground_truth: The name of the ground truth document.\n",
    "    :param retrieved_docs: The list of document names returned by the model in ranked order.\n",
    "    :param k: The cutoff for computing Precision.\n",
    "    :return: Precision at k.\n",
    "    \"\"\"\n",
    "    return 1 if ground_truth in retrieved_docs[:k] else 0\n",
    "\n",
    "def evaluate(doc_file, qa_pairs, ranking_fuction = None, k= 5):\n",
    "    \"\"\"\n",
    "    Evaluate the retrieval system based on the documents and question-answer pairs.\n",
    "\n",
    "    :param doc_file: Path to the document file.\n",
    "    :param qa_file: Path to the question-answer file.\n",
    "    :param k: The cutoff for Precision@k.\n",
    "    \"\"\"\n",
    "    # Load the QA pairs\n",
    "\n",
    "\n",
    "    precision_scores = []\n",
    "\n",
    "\n",
    "    for doc_id, question, _ in qa_pairs:\n",
    "\n",
    "        retrieved_docs = ranking_fuction(question)\n",
    "        precision_scores.append(precision_at_k(doc_id, retrieved_docs, k))\n",
    "\n",
    "        avg_precision_at_k = sum(precision_scores) / len(precision_scores)\n",
    "\n",
    "        if len(precision_scores) %10==0:\n",
    "            print(f\"After {len(precision_scores)} queries, Precision@{k}: {avg_precision_at_k}\")\n",
    "\n",
    "    # Compute average Precision@k\n",
    "    avg_precision_at_k = sum(precision_scores) / len(precision_scores)\n",
    "\n",
    "    print(f\"Precision@{k}: {avg_precision_at_k}\")\n",
    "\n",
    "\n",
    "qa_file = 'questions_answers.tsv'  # document ID <TAB> question <TAB> answer\n",
    "qa_pairs = load_questions_answers(qa_file)\n",
    "print(len(qa_pairs))\n",
    "qa_pairs[:3]\n",
    "\n",
    "# start_time = time.time()\n",
    "# evaluate(doc_file, qa_pairs,rank_documents_biencoder)\n",
    "# end_time = time.time()\n",
    "# elapsed_time = (end_time - start_time)/60\n",
    "# print(f\"Time taken: {elapsed_time} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation with batch_size=256, search_batch_size=512\n",
      "Encoding queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:18<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding completed in 18.8 seconds\n",
      "Performing batch similarity search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search completed in 3.9 seconds\n",
      "\n",
      "Performance Breakdown:\n",
      "- Encoding time: 18.8s (61.3 queries/s)\n",
      "- Search time: 3.9s (297.8 queries/s)\n",
      "\n",
      "Final Results:\n",
      "Precision@5: 0.467\n",
      "Total time: 22.7 seconds\n",
      "Average speed: 50.8 queries/second\n",
      "Total time: 0.38 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def batch_encode_queries_v2(queries, embedding_model, batch_size=256):\n",
    "    \"\"\"\n",
    "    Optimized batch encoding with larger batches and better GPU utilization\n",
    "    \"\"\"\n",
    "    # Pre-allocate memory for all embeddings\n",
    "    num_queries = len(queries)\n",
    "    embedding_dim = 384  # We know this from the output\n",
    "    all_embeddings = np.zeros((num_queries, embedding_dim), dtype=np.float32)\n",
    "    \n",
    "    # Process in larger batches\n",
    "    for i in tqdm(range(0, num_queries, batch_size), desc=\"Encoding queries\"):\n",
    "        end_idx = min(i + batch_size, num_queries)\n",
    "        batch = queries[i:end_idx]\n",
    "        \n",
    "        # Get embeddings for batch\n",
    "        batch_embeddings = embedding_model.embed_documents(batch)\n",
    "        all_embeddings[i:end_idx] = batch_embeddings\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "def evaluate_gpu_optimized_v2(qa_pairs, k=5, batch_size=256, search_batch_size=512):\n",
    "    \"\"\"\n",
    "    Further optimized GPU evaluation with separate batch sizes for encoding and search\n",
    "    \"\"\"\n",
    "    questions = [q for _, q, _ in qa_pairs]\n",
    "    ground_truths = [doc_id for doc_id, _, _ in qa_pairs]\n",
    "    \n",
    "    print(f\"Starting evaluation with batch_size={batch_size}, search_batch_size={search_batch_size}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Batch encode all queries\n",
    "    print(\"Encoding queries...\")\n",
    "    query_embeddings = batch_encode_queries_v2(questions, embedding_model, batch_size)\n",
    "    encoding_time = time.time() - start_time\n",
    "    print(f\"Encoding completed in {encoding_time:.1f} seconds\")\n",
    "    \n",
    "    # 2. Perform similarity search in batches\n",
    "    print(\"Performing batch similarity search...\")\n",
    "    search_start = time.time()\n",
    "    \n",
    "    all_D = []\n",
    "    all_I = []\n",
    "    num_queries = len(questions)\n",
    "    \n",
    "    for i in tqdm(range(0, num_queries, search_batch_size), desc=\"Searching\"):\n",
    "        end_idx = min(i + search_batch_size, num_queries)\n",
    "        batch_embeddings = query_embeddings[i:end_idx]\n",
    "        \n",
    "        # Perform search for this batch\n",
    "        D, I = KNOWLEDGE_VECTOR_DATABASE.index.search(batch_embeddings, k)\n",
    "        all_D.extend(D)\n",
    "        all_I.extend(I)\n",
    "    \n",
    "    search_time = time.time() - search_start\n",
    "    print(f\"Search completed in {search_time:.1f} seconds\")\n",
    "    \n",
    "    # 3. Process results\n",
    "    doc_dict = {i: doc.metadata['source'] for i, doc in enumerate(doc_snippets)}\n",
    "    retrieved_docs = [[doc_dict[idx] for idx in query_indices] for query_indices in all_I]\n",
    "    \n",
    "    # Calculate precision scores\n",
    "    precision_scores = [\n",
    "        1 if gt in retrieved[:k] else 0 \n",
    "        for gt, retrieved in zip(ground_truths, retrieved_docs)\n",
    "    ]\n",
    "    \n",
    "    # Final results\n",
    "    final_precision = np.mean(precision_scores)\n",
    "    total_time = time.time() - start_time\n",
    "    qps = num_queries / total_time\n",
    "    \n",
    "    print(\"\\nPerformance Breakdown:\")\n",
    "    print(f\"- Encoding time: {encoding_time:.1f}s ({num_queries/encoding_time:.1f} queries/s)\")\n",
    "    print(f\"- Search time: {search_time:.1f}s ({num_queries/search_time:.1f} queries/s)\")\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Precision@{k}: {final_precision:.3f}\")\n",
    "    print(f\"Total time: {total_time:.1f} seconds\")\n",
    "    print(f\"Average speed: {qps:.1f} queries/second\")\n",
    "    \n",
    "    return final_precision\n",
    "\n",
    "\n",
    "qa_file = 'questions_answers.tsv'\n",
    "qa_pairs = load_questions_answers(qa_file)\n",
    "\n",
    "start_time = time.time()\n",
    "# Use larger batch sizes\n",
    "score = evaluate_gpu_optimized_v2(\n",
    "    qa_pairs, \n",
    "    k=5, \n",
    "    batch_size=256,  # Encoding batch size\n",
    "    search_batch_size=512  # Search batch size\n",
    ")\n",
    "print(f\"Total time: {(time.time() - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Initial retrieval (batched)...\n",
      "Computing embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:11<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing batch search...\n",
      "\n",
      "2. ColBERT reranking (accurate mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:36<00:00,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Breakdown:\n",
      "Initial Retrieval: 11.6s (99.0 q/s)\n",
      "Reranking: 36.4s (31.7 q/s)\n",
      "\n",
      "Final Results:\n",
      "Precision@5: 0.475\n",
      "Total time: 48.0s (24.0 q/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "def batch_initial_retrieval_v2(questions, k=15, batch_size=512):\n",
    "    \"\"\"\n",
    "    More optimized batch retrieval using FAISS GPU\n",
    "    \"\"\"\n",
    "    # Convert FAISS index to GPU if not already\n",
    "    res = faiss.StandardGpuResources()\n",
    "    gpu_index = faiss.index_cpu_to_gpu(res, 0, KNOWLEDGE_VECTOR_DATABASE.index)\n",
    "    \n",
    "    # Pre-allocate embeddings array\n",
    "    num_queries = len(questions)\n",
    "    embedding_dim = KNOWLEDGE_VECTOR_DATABASE.index.d\n",
    "    all_embeddings = np.zeros((num_queries, embedding_dim), dtype=np.float32)\n",
    "    \n",
    "    print(\"Computing embeddings...\")\n",
    "    for i in tqdm(range(0, num_queries, batch_size)):\n",
    "        end_idx = min(i + batch_size, num_queries)\n",
    "        batch = questions[i:end_idx]\n",
    "        \n",
    "        # Get embeddings for batch\n",
    "        embeddings = embedding_model.embed_documents(batch)\n",
    "        all_embeddings[i:end_idx] = embeddings\n",
    "    \n",
    "    print(\"Performing batch search...\")\n",
    "    # Single batch search for all queries\n",
    "    D, I = gpu_index.search(all_embeddings, k)\n",
    "    \n",
    "    # Convert indices to documents (in batches)\n",
    "    retrieved_docs = []\n",
    "    for indices in I:\n",
    "        docs = [doc_snippets[idx] for idx in indices]\n",
    "        retrieved_docs.append(docs)\n",
    "    \n",
    "    return retrieved_docs\n",
    "\n",
    "def optimized_colbert_rerank_v3(questions, docs_list, batch_size=64):\n",
    "    \"\"\"\n",
    "    Optimized ColBERT reranking that maintains high precision\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    all_reranked = []\n",
    "    \n",
    "    # Process queries in batches\n",
    "    for i in tqdm(range(0, len(questions), batch_size), desc=\"Reranking\"):\n",
    "        batch_questions = questions[i:i + batch_size]\n",
    "        batch_docs = docs_list[i:i + batch_size]\n",
    "        \n",
    "        for q_idx, (question, docs) in enumerate(zip(batch_questions, batch_docs)):\n",
    "            # Get query embeddings\n",
    "            q_inputs = tokenizer(\n",
    "                question,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_embeds = model(**q_inputs).last_hidden_state  # [1, Q, D]\n",
    "            \n",
    "            # Process docs in mini-batches\n",
    "            doc_scores = []\n",
    "            doc_batch_size = 8  # Process multiple docs at once\n",
    "            \n",
    "            for j in range(0, len(docs), doc_batch_size):\n",
    "                doc_batch = docs[j:j + doc_batch_size]\n",
    "                doc_texts = [d.page_content for d in doc_batch]\n",
    "                \n",
    "                # Get document embeddings\n",
    "                d_inputs = tokenizer(\n",
    "                    doc_texts,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=512\n",
    "                ).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    d_embeds = model(**d_inputs).last_hidden_state  # [B, P, D]\n",
    "                \n",
    "                # Compute similarity scores for each document\n",
    "                for doc_idx in range(len(doc_batch)):\n",
    "                    # Extract single document embeddings\n",
    "                    d_embed = d_embeds[doc_idx:doc_idx+1]  # [1, P, D]\n",
    "                    \n",
    "                    # Compute token-level similarities\n",
    "                    sim_matrix = torch.matmul(q_embeds, d_embed.transpose(-2, -1))  # [1, Q, P]\n",
    "                    \n",
    "                    # MaxSim operation\n",
    "                    max_sim = torch.max(sim_matrix, dim=-1).values  # [1, Q]\n",
    "                    score = max_sim.sum().item()\n",
    "                    \n",
    "                    doc_scores.append((score, doc_batch[doc_idx].metadata['source']))\n",
    "            \n",
    "            # Sort and get top k docs\n",
    "            doc_scores.sort(reverse=True)\n",
    "            all_reranked.append([score[1] for score in doc_scores[:5]])\n",
    "    \n",
    "    return all_reranked\n",
    "\n",
    "def evaluate_optimized_v3(qa_pairs, initial_k=15, final_k=5, batch_size=512, rerank_batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluation with optimized but accurate reranking\n",
    "    \"\"\"\n",
    "    questions = [q for _, q, _ in qa_pairs]\n",
    "    ground_truths = [doc_id for doc_id, _, _ in qa_pairs]\n",
    "    \n",
    "    print(\"1. Initial retrieval (batched)...\")\n",
    "    init_start = time.time()\n",
    "    initial_retrieved = batch_initial_retrieval_v2(\n",
    "        questions, \n",
    "        k=initial_k, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    init_time = time.time() - init_start\n",
    "    \n",
    "    print(\"\\n2. ColBERT reranking (accurate mode)...\")\n",
    "    rerank_start = time.time()\n",
    "    reranked_docs = optimized_colbert_rerank_v3(\n",
    "        questions, \n",
    "        initial_retrieved, \n",
    "        batch_size=rerank_batch_size\n",
    "    )\n",
    "    rerank_time = time.time() - rerank_start\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision_scores = [\n",
    "        1 if gt in reranked[:final_k] else 0 \n",
    "        for gt, reranked in zip(ground_truths, reranked_docs)\n",
    "    ]\n",
    "    \n",
    "    # Results\n",
    "    final_precision = np.mean(precision_scores)\n",
    "    total_time = time.time() - init_start\n",
    "    \n",
    "    print(\"\\nPerformance Breakdown:\")\n",
    "    print(f\"Initial Retrieval: {init_time:.1f}s ({len(questions)/init_time:.1f} q/s)\")\n",
    "    print(f\"Reranking: {rerank_time:.1f}s ({len(questions)/rerank_time:.1f} q/s)\")\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Precision@{final_k}: {final_precision:.3f}\")\n",
    "    print(f\"Total time: {total_time:.1f}s ({len(questions)/total_time:.1f} q/s)\")\n",
    "    \n",
    "    return final_precision\n",
    "\n",
    "# Test with same set for comparison\n",
    "score = evaluate_optimized_v3(\n",
    "    qa_pairs,\n",
    "    batch_size=512,           # Initial retrieval batch size\n",
    "    rerank_batch_size=64      # Reranking batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "@dataclass\n",
    "class PromptTemplate:\n",
    "    template: str\n",
    "    input_variables: List[str]\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "class PromptManager:\n",
    "    def __init__(self):\n",
    "        self.templates = {\n",
    "            # Zero-shot prompting\n",
    "            \"basic\": PromptTemplate(\n",
    "                template=\"Answer the question based on the given context.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\",\n",
    "                input_variables=[\"context\", \"question\"]\n",
    "            ),\n",
    "            \n",
    "            # Chain-of-thought prompting\n",
    "            \"cot\": PromptTemplate(\n",
    "                template=\"Let's approach this step-by-step:\\n\\n1) First, understand the question: {question}\\n\\n2) Here's the relevant context: {context}\\n\\n3) Let's analyze the context and break down the key points\\n\\n4) Based on this analysis, provide a detailed answer.\\n\\nReasoning and answer:\",\n",
    "                input_variables=[\"context\", \"question\"]\n",
    "            ),\n",
    "            \n",
    "            # Role-based prompting\n",
    "            \"expert\": PromptTemplate(\n",
    "                template=\"As an expert in meeting analysis, review the following context and answer the question.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nExpert analysis and answer:\",\n",
    "                input_variables=[\"context\", \"question\"]\n",
    "            ),\n",
    "            \n",
    "            # Self-reflection prompting\n",
    "            \"reflective\": PromptTemplate(\n",
    "                template=\"Question: {question}\\n\\nContext: {context}\\n\\nLet me think about this carefully:\\n1. What are the key points in the context?\\n2. How do they relate to the question?\\n3. What might I be missing?\\n\\nConsidering these points, here's my answer:\",\n",
    "                input_variables=[\"context\", \"question\"]\n",
    "            ),\n",
    "            \n",
    "            # Structured output prompting\n",
    "            \"structured\": PromptTemplate(\n",
    "                template=\"Based on the context below, provide a structured answer to the question.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer in the following format:\\n- Main point:\\n- Supporting details:\\n- Additional context:\\n- Confidence level:\\n\",\n",
    "                input_variables=[\"context\", \"question\"]\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def get_prompt(self, style: str, **kwargs) -> str:\n",
    "        if style not in self.templates:\n",
    "            raise ValueError(f\"Unknown prompt style: {style}\")\n",
    "        return self.templates[style].format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Reader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     top_p: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m\n\u001b[0;32m     17\u001b[0m     prompt_template: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer the question based on the given context.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mContext: \u001b[39m\u001b[38;5;132;01m{context}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEnhancedReader\u001b[39;00m(\u001b[43mReader\u001b[49m):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: ReaderConfig):\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Reader' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Optional\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  \n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ReaderConfig:\n",
    "    \"\"\"Configuration for the Reader component\"\"\"\n",
    "    model_name: str = \"google/flan-t5-base\"  # Can also use larger variants\n",
    "    max_source_length: int = 1024\n",
    "    max_target_length: int = 256\n",
    "    num_beams: int = 4\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    temperature: float = 0.7\n",
    "    do_sample: bool = True\n",
    "    top_p: float = 0.95\n",
    "    prompt_template: str = \"Answer the question based on the given context.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "\n",
    "class EnhancedReader(Reader):\n",
    "    def __init__(self, config: ReaderConfig):\n",
    "        super().__init__(config)\n",
    "        self.prompt_manager = PromptManager()\n",
    "        \n",
    "    def generate_answer_with_prompt_style(\n",
    "        self, \n",
    "        question: str, \n",
    "        retrieved_docs: List[str],\n",
    "        prompt_style: str = \"basic\",\n",
    "        return_context: bool = False\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Generate an answer using a specific prompting strategy\n",
    "        \"\"\"\n",
    "        context = self._prepare_context(retrieved_docs, self.config.max_source_length)\n",
    "        \n",
    "        # Get the appropriate prompt\n",
    "        prompt = self.prompt_manager.get_prompt(\n",
    "            style=prompt_style,\n",
    "            context=context,\n",
    "            question=question\n",
    "        )\n",
    "        \n",
    "        # Generate answer with the selected prompt\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=self.config.max_source_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.config.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=self.config.max_target_length,\n",
    "                num_beams=self.config.num_beams,\n",
    "                temperature=self.config.temperature,\n",
    "                do_sample=self.config.do_sample,\n",
    "                top_p=self.config.top_p\n",
    "            )\n",
    "        \n",
    "        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        result = {\n",
    "            \"answer\": answer,\n",
    "            \"prompt_style\": prompt_style\n",
    "        }\n",
    "        \n",
    "        if return_context:\n",
    "            result[\"context\"] = context\n",
    "            result[\"full_prompt\"] = prompt\n",
    "            \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_config = ReaderConfig()\n",
    "reader = EnhancedReader(config=reader_config)\n",
    "\n",
    "rag_output = reader.generate_answer(user_query, retrieved_docs, return_context=True)\n",
    "rag_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "class RAGQAEvaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the RAG QA evaluation system.\"\"\"\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        try:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        except:\n",
    "            print(\"Please install spaCy model: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "    def evaluate_answer(self, \n",
    "                       rag_output: Dict[str, str],\n",
    "                       qa_pairs: List[Tuple[str, str, str]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate RAG output against ground truth QA pairs.\n",
    "        \n",
    "        Args:\n",
    "            rag_output: Dictionary containing 'answer' and 'context' from RAG\n",
    "            qa_pairs: List of tuples (doc_id, question, answer) from ground truth\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Extract mentioned documents from RAG context\n",
    "        mentioned_docs = set(rag_output['context'].split())\n",
    "        \n",
    "        # Find matching QA pairs based on document overlap\n",
    "        matching_pairs = [\n",
    "            (q, a) for doc_id, q, a in qa_pairs \n",
    "            if doc_id in mentioned_docs\n",
    "        ]\n",
    "        \n",
    "        if not matching_pairs:\n",
    "            return {\n",
    "                'error': 'No matching ground truth QA pairs found for the retrieved documents',\n",
    "                'retrieved_docs': list(mentioned_docs)\n",
    "            }\n",
    "        \n",
    "        # Calculate metrics for the generated answer against all matching pairs\n",
    "        answer_metrics = []\n",
    "        for question, answer in matching_pairs:\n",
    "            pair_metrics = self._calculate_metrics(\n",
    "                generated_answer=rag_output['answer'],\n",
    "                reference_question=question,\n",
    "                reference_answer=answer\n",
    "            )\n",
    "            answer_metrics.append(pair_metrics)\n",
    "        \n",
    "        # Take the best scores across all matching pairs\n",
    "        metrics['best_match'] = {\n",
    "            metric: max(m[metric] for m in answer_metrics)\n",
    "            for metric in answer_metrics[0].keys()\n",
    "        }\n",
    "        \n",
    "        # Calculate relevance to retrieved context\n",
    "        if 'context' in rag_output:\n",
    "            context_doc = self.nlp(' '.join(rag_output['context'].split()))\n",
    "            answer_doc = self.nlp(rag_output['answer'])\n",
    "            metrics['context_relevance'] = context_doc.similarity(answer_doc)\n",
    "        \n",
    "        # Add document coverage metrics\n",
    "        metrics['document_coverage'] = {\n",
    "            'num_retrieved': len(mentioned_docs),\n",
    "            'retrieved_docs': list(mentioned_docs)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def _calculate_metrics(self, \n",
    "                         generated_answer: str,\n",
    "                         reference_question: str, \n",
    "                         reference_answer: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate various similarity metrics between generated and reference text.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # ROUGE scores\n",
    "        rouge_scores = self.rouge_scorer.score(reference_answer, generated_answer)\n",
    "        metrics['rouge1_f1'] = rouge_scores['rouge1'].fmeasure\n",
    "        metrics['rouge2_f1'] = rouge_scores['rouge2'].fmeasure\n",
    "        metrics['rougeL_f1'] = rouge_scores['rougeL'].fmeasure\n",
    "        \n",
    "        # BLEU score\n",
    "        metrics['bleu'] = sentence_bleu(\n",
    "            [reference_answer.split()],\n",
    "            generated_answer.split()\n",
    "        )\n",
    "        \n",
    "        # Semantic similarity\n",
    "        ref_answer_doc = self.nlp(reference_answer)\n",
    "        gen_answer_doc = self.nlp(generated_answer)\n",
    "        metrics['semantic_similarity'] = ref_answer_doc.similarity(gen_answer_doc)\n",
    "        \n",
    "        # Question relevance\n",
    "        ref_question_doc = self.nlp(reference_question)\n",
    "        metrics['question_relevance'] = gen_answer_doc.similarity(ref_question_doc)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def get_evaluation_summary(self, metrics: Dict) -> str:\n",
    "        \"\"\"Generate a human-readable summary of the evaluation metrics.\"\"\"\n",
    "        if 'error' in metrics:\n",
    "            return f\"Error: {metrics['error']}\\nRetrieved documents: {', '.join(metrics['retrieved_docs'])}\"\n",
    "        \n",
    "        summary = []\n",
    "        \n",
    "        if 'best_match' in metrics:\n",
    "            summary.append(\"Best Matching Scores:\")\n",
    "            summary.append(f\"Content Overlap:\")\n",
    "            summary.append(f\"- ROUGE-1 F1: {metrics['best_match']['rouge1_f1']:.3f}\")\n",
    "            summary.append(f\"- ROUGE-2 F1: {metrics['best_match']['rouge2_f1']:.3f}\")\n",
    "            summary.append(f\"- ROUGE-L F1: {metrics['best_match']['rougeL_f1']:.3f}\")\n",
    "            summary.append(f\"- BLEU Score: {metrics['best_match']['bleu']:.3f}\")\n",
    "            summary.append(f\"\\nSemantic Evaluation:\")\n",
    "            summary.append(f\"- Semantic Similarity: {metrics['best_match']['semantic_similarity']:.3f}\")\n",
    "            summary.append(f\"- Question Relevance: {metrics['best_match']['question_relevance']:.3f}\")\n",
    "        \n",
    "        if 'context_relevance' in metrics:\n",
    "            summary.append(f\"\\nContext Relevance: {metrics['context_relevance']:.3f}\")\n",
    "        \n",
    "        if 'document_coverage' in metrics:\n",
    "            summary.append(f\"\\nDocument Coverage:\")\n",
    "            summary.append(f\"- Number of Retrieved Documents: {metrics['document_coverage']['num_retrieved']}\")\n",
    "            summary.append(f\"- Retrieved Documents: {', '.join(metrics['document_coverage']['retrieved_docs'])}\")\n",
    "        \n",
    "        return '\\n'.join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluator\n",
    "evaluator = RAGQAEvaluator()\n",
    "\n",
    "# Get evaluation metrics\n",
    "metrics = evaluator.evaluate_answer(rag_output, qa_pairs)\n",
    "\n",
    "# Print human-readable summary\n",
    "print(evaluator.get_evaluation_summary(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cse256pa1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01c4b26169344756a5004ae29d7ae15e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "01f33dd22cde4935b251fc1ccec28b06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dbaa70ee635e4e8ebdf55c00449ba804",
       "IPY_MODEL_cdbfbc4cf6f74e7e885c147e3aa2f33c",
       "IPY_MODEL_9aa28e63d210437e87bbad2c5faea550"
      ],
      "layout": "IPY_MODEL_424c827f13d346d7a1aef2e40b3ed31d"
     }
    },
    "0e9edef7b1bc4fddac576d1d6b9080ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_878da6d48d014346bb2460d0276a1647",
      "max": 18070,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b01d2f8e35ca4c0c993c70fa59dec499",
      "value": 18070
     }
    },
    "19ba855ec2404856a2cba7ad451b9021": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4ad90e60abd40a481bf4eed0e4bdf54",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_01c4b26169344756a5004ae29d7ae15e",
      "value": "‚Äá18070/18070‚Äá[00:24&lt;00:00,‚Äá1796.24it/s]"
     }
    },
    "21da30f58191447a8a7a0daa08ace10b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "424c827f13d346d7a1aef2e40b3ed31d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a1b783c75ab49af9e428085777d5c2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66e6956c50b64d9ba8fb03e8fc38471c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75768aac5c9c420e88796c9637557d7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9708a67aa94a4b0babd63ce89a328d11",
       "IPY_MODEL_0e9edef7b1bc4fddac576d1d6b9080ea",
       "IPY_MODEL_19ba855ec2404856a2cba7ad451b9021"
      ],
      "layout": "IPY_MODEL_7a33f51157164324a9bf871af54e2140"
     }
    },
    "7a33f51157164324a9bf871af54e2140": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "878da6d48d014346bb2460d0276a1647": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "885282abe8984dd589b5e8b271c70351": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9708a67aa94a4b0babd63ce89a328d11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_faf92c79609f41e78b82b2b6f6e9e354",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_21da30f58191447a8a7a0daa08ace10b",
      "value": "100%"
     }
    },
    "9aa28e63d210437e87bbad2c5faea550": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_885282abe8984dd589b5e8b271c70351",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c54bd6023b894bc18350e7f99cb56b3e",
      "value": "‚Äá230/230‚Äá[00:19&lt;00:00,‚Äá16.52it/s]"
     }
    },
    "aa567554981c449e9a19d2cc80a15a18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b01d2f8e35ca4c0c993c70fa59dec499": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b81068c5e79f4697b1ec9033c2472a2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c54bd6023b894bc18350e7f99cb56b3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cdbfbc4cf6f74e7e885c147e3aa2f33c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66e6956c50b64d9ba8fb03e8fc38471c",
      "max": 230,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b81068c5e79f4697b1ec9033c2472a2f",
      "value": 230
     }
    },
    "d4ad90e60abd40a481bf4eed0e4bdf54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbaa70ee635e4e8ebdf55c00449ba804": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa567554981c449e9a19d2cc80a15a18",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5a1b783c75ab49af9e428085777d5c2b",
      "value": "100%"
     }
    },
    "faf92c79609f41e78b82b2b6f6e9e354": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
